{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bc2d98-ece2-40f0-a95b-50d4562b1407",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "844efcdb-e088-4ed8-b601-c21c1df975f3",
       "queued_time": "2024-11-12T20:08:39.134475Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (4.37.2)\r\n",
      "Collecting transformers\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (3.11.0)\r\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\r\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (0.4.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\r\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/clonedenv/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\r\n",
      "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: huggingface-hub, tokenizers, transformers\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface_hub 0.23.1\r\n",
      "    Uninstalling huggingface_hub-0.23.1:\r\n",
      "      Successfully uninstalled huggingface_hub-0.23.1\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.15.1\r\n",
      "    Uninstalling tokenizers-0.15.1:\r\n",
      "      Successfully uninstalled tokenizers-0.15.1\r\n",
      "  Attempting uninstall: transformers\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: transformers 4.37.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling transformers-4.37.2:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled transformers-4.37.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed huggingface-hub-0.26.2 tokenizers-0.20.3 transformers-4.46.2\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, TimestampType\n",
    "import pandas as pd\n",
    "# Define schema for flattened research paper data with additional fields\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('hash_id', LongType(), True),  # Unique hash ID for the paper\n",
    "        StructField('title', StringType(), True),  # Title of the paper\n",
    "        StructField('authors', StringType(), True),  # Authors of the paper as a comma-separated string\n",
    "        StructField('published', TimestampType(), True),  # Publication date of the paper\n",
    "        StructField('summary', StringType(), True),  # Abstract or summary of the paper\n",
    "        StructField('pdf_url', StringType(), True),  # URL to download the paper's PDF\n",
    "        StructField('entry_id', StringType(), True),  # Unique entry ID for the paper (e.g., arXiv ID)\n",
    "        StructField('recommended', LongType(), True),  # Flag indicating if the paper is recommended (1 if recommended, 0 otherwise)\n",
    "        StructField('referenceCount', LongType(), True),  # Number of references in the paper\n",
    "        StructField('citationCount', LongType(), True),  # Number of times the paper has been cited\n",
    "        StructField('references', StringType(), True),  # JSON string of references\n",
    "        StructField('citations', StringType(), True),  # JSON string of citations\n",
    "        StructField('s2FieldsOfStudy', StringType(), True),  # JSON string of fields of study\n",
    "        StructField('tldr', StringType(), True),  # JSON string for TLDR summary\n",
    "        StructField('query_id', StringType(), True),  # Identifier for the query that generated the result\n",
    "        StructField('Tag_1', StringType(), True),  # Additional tag 1\n",
    "        StructField('Tag_2', StringType(), True),  # Additional tag 2\n",
    "        StructField('Tag_3', StringType(), True),  # Additional tag 3\n",
    "        StructField('Tag_4', StringType(), True),  # Additional tag 4\n",
    "        StructField('Tag_5', StringType(), True),  # Additional tag 5\n",
    "        StructField('field', StringType(), True)  # Field of the paper\n",
    "    ]\n",
    ")\n",
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50ffce2-8171-4a4f-b777-4058b3e71bd5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "f0522979-3fd9-4111-b13a-194ab4d866dd",
       "queued_time": "2024-11-12T19:10:33.6797958Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM query.query_processing LIMIT 10000\").dropDuplicates([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31541e4-bf1c-43cd-aa87-61d9cf875c55",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "ca15dc4c-4853-4197-bfca-ca04ad35c975",
       "queued_time": "2024-11-12T19:10:35.1488846Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"query.query_processing\")\n",
    "df = spark.sql(\"SELECT * FROM query.query_processing LIMIT 10000\")\n",
    "df_pd = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff80b20-138e-4626-acee-2850b6dae980",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "9c618bd4-f51a-48a0-9167-ffcb8809904d",
       "queued_time": "2024-11-12T19:18:13.3607879Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.mssparkutilsrunmultiple-result+json": {
       "activities": [
        {
         "activity_name": "query_Machine Learning",
         "args": {},
         "artifact_id": "",
         "capacity_id": "",
         "end_time": 0,
         "exception": "",
         "in_pipeline": "",
         "notebook_name": "arxiv-search",
         "progress": 0,
         "root_artifact_id": "",
         "run_id": "",
         "session_id": "",
         "snapshot_error": "",
         "snapshot_status": "pending",
         "spark_pool": "",
         "start_time": 0,
         "status": "pending",
         "status_msg": "Pending",
         "workspace_id": ""
        }
       ],
       "limit": 50,
       "numbers": {
        "failed": 0,
        "pending": 1,
        "running": 0,
        "succeeded": -1
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"/usr/lib/miniforge3/pkgs/graphviz-2.50.0-h1b29801_1/bin/../lib/graphviz/libgvplugin_pango.so.6\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"166pt\" height=\"44pt\" viewBox=\"0.00 0.00 165.89 44.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 161.89,-40 161.89,4 -4,4\"/>\n",
       "<!-- query_Machine Learning -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>query_Machine Learning</title>\n",
       "<g id=\"a_node1\"><a xlink:title=\"Name: query_Machine Learning\n",
       "Args: paper_num=10\n",
       "  query=Machine Learning\n",
       "  query_id=1c8001a6e68b47e2a04627531d939bb3\n",
       "  date_to=2024-05-25\n",
       "  date_from=2024-03-10\n",
       "Time: 2024-11-12 19:18:15 -&gt; 2024-11-12 19:19:49, Duration: 93.988s\n",
       "Status: Success (100%)\">\n",
       "<polygon fill=\"none\" stroke=\"#107c10\" points=\"157.83,-36 0.06,-36 0.06,0 157.83,0 157.83,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"78.94\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">query_Machine Learning</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.mssparkutilsrunmultiple-result+json": {
       "activities": [
        {
         "activity_name": "query_Federated Learning & Privacy-Preserving ML",
         "args": {},
         "artifact_id": "",
         "capacity_id": "",
         "end_time": 0,
         "exception": "",
         "in_pipeline": "",
         "notebook_name": "arxiv-search",
         "progress": 0,
         "root_artifact_id": "",
         "run_id": "",
         "session_id": "",
         "snapshot_error": "",
         "snapshot_status": "pending",
         "spark_pool": "",
         "start_time": 0,
         "status": "pending",
         "status_msg": "Pending",
         "workspace_id": ""
        },
        {
         "activity_name": "query_Causal Inference",
         "args": {},
         "artifact_id": "",
         "capacity_id": "",
         "end_time": 0,
         "exception": "",
         "in_pipeline": "",
         "notebook_name": "arxiv-search",
         "progress": 0,
         "root_artifact_id": "",
         "run_id": "",
         "session_id": "",
         "snapshot_error": "",
         "snapshot_status": "pending",
         "spark_pool": "",
         "start_time": 0,
         "status": "pending",
         "status_msg": "Pending",
         "workspace_id": ""
        },
        {
         "activity_name": "query_Self-Supervised Learning",
         "args": {},
         "artifact_id": "",
         "capacity_id": "",
         "end_time": 0,
         "exception": "",
         "in_pipeline": "",
         "notebook_name": "arxiv-search",
         "progress": 0,
         "root_artifact_id": "",
         "run_id": "",
         "session_id": "",
         "snapshot_error": "",
         "snapshot_status": "pending",
         "spark_pool": "",
         "start_time": 0,
         "status": "pending",
         "status_msg": "Pending",
         "workspace_id": ""
        },
        {
         "activity_name": "query_Privacy-Preserving Federated Learning",
         "args": {},
         "artifact_id": "",
         "capacity_id": "",
         "end_time": 0,
         "exception": "",
         "in_pipeline": "",
         "notebook_name": "arxiv-search",
         "progress": 0,
         "root_artifact_id": "",
         "run_id": "",
         "session_id": "",
         "snapshot_error": "",
         "snapshot_status": "pending",
         "spark_pool": "",
         "start_time": 0,
         "status": "pending",
         "status_msg": "Pending",
         "workspace_id": ""
        },
        {
         "activity_name": "query_Differential privacy in clinical and epidemiological research",
         "args": {},
         "artifact_id": "",
         "capacity_id": "",
         "end_time": 0,
         "exception": "",
         "in_pipeline": "",
         "notebook_name": "arxiv-search",
         "progress": 0,
         "root_artifact_id": "",
         "run_id": "",
         "session_id": "",
         "snapshot_error": "",
         "snapshot_status": "pending",
         "spark_pool": "",
         "start_time": 0,
         "status": "pending",
         "status_msg": "Pending",
         "workspace_id": ""
        }
       ],
       "limit": 50,
       "numbers": {
        "failed": 0,
        "pending": 5,
        "running": 0,
        "succeeded": -1
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activities = []\n",
    "for _,row in df_pd.iterrows():\n",
    "    _id = row[\"id\"]\n",
    "    for q in eval(row[\"query_text\"]):\n",
    "        q = q.strip()\n",
    "        activities.append({\n",
    "            \"name\": f\"query_{q}\", # activity name, must be unique\n",
    "            \"path\": \"arxiv-search\", # notebook path\n",
    "            \"timeoutPerCellInSeconds\": 10000, # max timeout for each cell, default to 90 seconds\n",
    "            \"args\": {\"query_id\": row[\"id\"], \"query\": q , \"paper_num\":1\n",
    "             , \"date_from\":row[\"date_from\"] , 'date_to':row[\"date_to\"]}, # notebook parameters\n",
    "        })\n",
    "        # run multiple notebooks with parameters\n",
    "    DAG = {\n",
    "        \"activities\": activities,\n",
    "        \"timeoutInSeconds\": 43200, # max timeout for the entire DAG, default to 12 hours\n",
    "        \"concurrency\": 50 # max number of notebooks to run concurrently, default to 50\n",
    "    }\n",
    "    mssparkutils.notebook.runMultiple(DAG, {\"displayDAGViaGraphviz\": True})\n",
    "    tables = spark.catalog.listTables(\"silver\")\n",
    "    id_tables = ['silver.'+table.name for table in tables if table.name.startswith(f'_{_id}')]\n",
    "    merged_df = spark.createDataFrame([],schema=schema)\n",
    "    for table_name in id_tables:\n",
    "        temp_df = spark.table(table_name)\n",
    "        merged_df = merged_df.union(temp_df)\n",
    "    merged_df = merged_df.distinct()\n",
    "    merged_df.write.mode(\"overwrite\").saveAsTable(f\"silver._{_id}\")\n",
    "    activities = []\n",
    "    # merged_df.write.mode(\"append\").saveAsTable(f\"silver.target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "540dd3db-9832-4d6c-8389-f66262070eff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T21:04:49.9821315Z",
       "execution_start_time": "2024-11-10T21:04:47.5086251Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "93387285-b02b-4281-b93f-f47bf6e21655",
       "queued_time": "2024-11-10T21:04:47.1087749Z",
       "session_id": "924123a7-886a-427a-ab89-9805258257fb",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 42,
       "statement_ids": [
        42
       ]
      },
      "text/plain": [
       "StatementMeta(, 924123a7-886a-427a-ab89-9805258257fb, 42, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5d0b4a3-91bf-4e80-b7b0-6f5e384f323f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T21:18:16.3139737Z",
       "execution_start_time": "2024-11-10T21:18:11.3766795Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "50d907f0-ea8c-4977-984c-fee227112e29",
       "queued_time": "2024-11-10T21:18:10.9625241Z",
       "session_id": "924123a7-886a-427a-ab89-9805258257fb",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 45,
       "statement_ids": [
        45
       ]
      },
      "text/plain": [
       "StatementMeta(, 924123a7-886a-427a-ab89-9805258257fb, 45, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document upserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from email.policy import default\n",
    "\n",
    "from azure.cosmos import CosmosClient, PartitionKey, exceptions\n",
    "from typing import List, Dict, Any, Optional\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import hmac\n",
    "import base64\n",
    "from datetime import datetime , timezone\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, TimestampType, LongType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"query_text\", StringType(), True),\n",
    "    StructField(\"date_from\", StringType(), True),\n",
    "    StructField(\"date_to\", StringType(), True),\n",
    "    StructField(\"priority\", LongType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"status\", LongType(), True),\n",
    "    StructField(\"metadata.created_at\", TimestampType(), True),\n",
    "    StructField(\"metadata.updated_at\", TimestampType(), True),\n",
    "    StructField(\"metadata.source\", StringType(), True),\n",
    "    StructField(\"partitionKey\", StringType(), True),\n",
    "    StructField(\"_rid\", StringType(), True),\n",
    "    StructField(\"_self\", StringType(), True),\n",
    "    StructField(\"_etag\", StringType(), True),\n",
    "    StructField(\"_attachments\", StringType(), True),\n",
    "    StructField(\"updated_at\", StringType(), True),\n",
    "    StructField(\"_ts\", LongType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "class CosmosDBClient:\n",
    "    def __init__(self, url: str, key: str, database_name: str, container_name: str, partition_key: str):\n",
    "        \"\"\"\n",
    "        Initialize the CosmosDBClient with Cosmos DB URL, key, database, container, and partition key.\n",
    "\n",
    "        Parameters:\n",
    "            url (str): Cosmos DB endpoint URL.\n",
    "            key (str): Cosmos DB primary key for authorization.\n",
    "            database_name (str): Name of the Cosmos DB database.\n",
    "            container_name (str): Name of the Cosmos DB container.\n",
    "            partition_key (str): Partition key path for the container.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.key = key\n",
    "        self.database_name = database_name\n",
    "        self.container_name = container_name\n",
    "        self.partition_key = partition_key\n",
    "\n",
    "        # Initialize Cosmos Client and connect to database and container\n",
    "        try:\n",
    "            self.client = CosmosClient(self.url, credential=self.key)\n",
    "            self.database = self._create_database_if_not_exists()\n",
    "            self.container = self._create_container_if_not_exists()\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Failed to connect to Cosmos DB: {e}\")\n",
    "\n",
    "    def _create_database_if_not_exists(self):\n",
    "        \"\"\"\n",
    "        Create the database if it does not exist.\n",
    "\n",
    "        Returns:\n",
    "            DatabaseProxy: A reference to the Cosmos DB database.\n",
    "        \"\"\"\n",
    "        return self.client.create_database_if_not_exists(id=self.database_name)\n",
    "\n",
    "    def _create_container_if_not_exists(self):\n",
    "        \"\"\"\n",
    "        Create the container if it does not exist with a specified partition key.\n",
    "\n",
    "        Returns:\n",
    "            ContainerProxy: A reference to the Cosmos DB container.\n",
    "        \"\"\"\n",
    "        return self.database.create_container_if_not_exists(\n",
    "            id=self.container_name,\n",
    "            partition_key=PartitionKey(path=f\"/{self.partition_key}\"),\n",
    "            offer_throughput=400  # Set the desired throughput\n",
    "        )\n",
    "\n",
    "    def create_document(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Insert a document into the container. ID will be autogenerated if not provided.\n",
    "\n",
    "        Parameters:\n",
    "            data (Dict[str, Any]): Document data to insert.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: The created document, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data.setdefault(self.partition_key, \"default_partition\")  # Ensure partition key exists\n",
    "            document = self.container.create_item(body=data)\n",
    "            print(\"Document created successfully.\")\n",
    "            return document\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error creating document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def upsert_document(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Upsert (update or insert) a document in the container.\n",
    "\n",
    "        Parameters:\n",
    "            data (Dict[str, Any]): Document data to upsert.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: The upserted document, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data.setdefault(self.partition_key, \"default_partition\")\n",
    "            document = self.container.upsert_item(body=data)\n",
    "            print(\"Document upserted successfully.\")\n",
    "            return document\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error upserting document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def read_document(self, document_id: str, partition_key: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Read a document by ID.\n",
    "\n",
    "        Parameters:\n",
    "            document_id (str): The ID of the document to read.\n",
    "            partition_key (str): The partition key of the document.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: The retrieved document, or None if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            document = self.container.read_item(item=document_id, partition_key=partition_key)\n",
    "            return document\n",
    "        except exceptions.CosmosResourceNotFoundError:\n",
    "            print(\"Document not found.\")\n",
    "            return None\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error reading document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def query_documents(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Query documents in the container.\n",
    "\n",
    "        Parameters:\n",
    "            query (str): SQL query string to execute.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of documents that match the query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            items = list(self.container.query_items(query=query, enable_cross_partition_query=True))\n",
    "            return items\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error querying documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def delete_document(self, document_id: str, partition_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete a document by ID.\n",
    "\n",
    "        Parameters:\n",
    "            document_id (str): The ID of the document to delete.\n",
    "            partition_key (str): The partition key of the document.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the document was deleted, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.container.delete_item(item=document_id, partition_key=partition_key)\n",
    "            print(\"Document deleted successfully.\")\n",
    "            return True\n",
    "        except exceptions.CosmosResourceNotFoundError:\n",
    "            print(\"Document not found.\")\n",
    "            return False\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error deleting document: {e}\")\n",
    "            return False\n",
    "\n",
    "    def list_all_documents(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List all documents in the container.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of all documents in the container.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            items = list(self.container.read_all_items())\n",
    "            return items\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error listing all documents: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "COSMOS_URL = ''\n",
    "COSMOS_KEY = \"\"\n",
    "DATABASE_NAME = ''\n",
    "CONTAINER_NAME = ''\n",
    "PARTITION_KEY = ''  # Ensure this matches the partition key path in your Cosmos DB\n",
    "\n",
    "USER_ID = \"\"\n",
    "\n",
    "cosmos_client_query = CosmosDBClient(\n",
    "    url=COSMOS_URL,\n",
    "    key=COSMOS_KEY,\n",
    "    database_name=DATABASE_NAME,\n",
    "    container_name=CONTAINER_NAME,\n",
    "    partition_key=PARTITION_KEY\n",
    ")\n",
    "def appendTable(source,name):\n",
    "    spark_df = spark.createDataFrame(source,schema)\n",
    "    spark_df.write.mode(\"append\").format(\"delta\").saveAsTable(name)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(\"SELECT * FROM query.query_processing  WHERE status=1 ORDER BY priority LIMIT 1000\")\n",
    "except:\n",
    "    appendTable([],\"query_completed\")\n",
    "    df = spark.sql(\"SELECT * FROM query.query_processing  WHERE status=1 ORDER BY priority LIMIT 1000\")\n",
    "\n",
    "\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "\n",
    "\n",
    "for _id, user_id in zip(df_pd[\"id\"], df_pd[\"user_id\"]):\n",
    "    # Query to get the specific document by its ID and user_id (partition key)\n",
    "    query = f\"SELECT * FROM c WHERE c.id = '{_id}' AND c.user_id = '{user_id}'\"\n",
    "    documents = cosmos_client_query.query_documents(query=query)\n",
    "    \n",
    "    if documents:\n",
    "        document = documents[0]  # Assuming each ID corresponds to a single document\n",
    "        \n",
    "        # Update the status and add a timestamp\n",
    "        document['status'] = 2\n",
    "        document['updated_at'] = datetime.now(timezone.utc).isoformat()  # Optional timestamp\n",
    "        \n",
    "        # Upsert the updated document back into Cosmos DB\n",
    "        cosmos_client_query.upsert_document(document)\n",
    "\n",
    "\n",
    "df_pd[\"status\"] =2\n",
    "\n",
    "try:\n",
    "    appendTable(df_pd,\"query.query_completed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS query.query_processing\")\n",
    "empty_pending = spark.createDataFrame([], schema)\n",
    "appendTable([],\"query.query_processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827f22ad-bd43-479c-9477-9b675df07e49",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "b449d098-6a6d-478d-9939-b8e0aa8115f0",
       "queued_time": "2024-11-12T20:09:51.6536894Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# t = spark.catalog.listTables(\"silver\")\n",
    "# for i in t:\n",
    "#     if(i.name.startswith(\"_\")):\n",
    "#         spark.sql(f\"DROP TABLE silver.{i.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370de7ab-0de8-44a1-a26e-fed9afbf5445",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "3bba3bc1-5bbe-490d-9ec1-3dc79c187553",
       "queued_time": "2024-11-12T09:37:37.5374028Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExitValue: {'query_filename_list': [{'email': '101540385@georgebrown.ca', 'id': '1c8001a6e68b47e2a04627531d939bb3', 'query_text': '[\"Machine Learning\"]', 'date_from': '2024-03-10', 'date_to': '2024-05-25', 'user_id': 'Alan Choi'}, {'email': '101540385@georgebrown.ca', 'id': 'ecf967251e6747248e6e79aa564301c2', 'query_text': '[\"Robotics\"]', 'date_from': '2024-06-19', 'date_to': '2024-08-23', 'user_id': 'Alan Choi'}, {'email': '101540385@georgebrown.ca', 'id': 'fc827af370664e82919b0b3db4f856ca', 'query_text': '[\"Large Language Model\"]', 'date_from': '2024-05-10', 'date_to': '2024-10-17', 'user_id': 'Alan Choi'}]}"
     ]
    }
   ],
   "source": [
    "lResult = {\n",
    "    \"query_filename_list\": [\n",
    "        {\n",
    "            \"email\": email,\n",
    "            \"id\":_id,\n",
    "            \"query_text\": query_text,\n",
    "            \"date_from\": date_from,\n",
    "            \"date_to\": date_to,\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "        for email, _id , query_text, date_from, date_to,user_id,priority in zip(\n",
    "            df_pd[\"email\"], df_pd[\"id\"] , df_pd[\"query_text\"], df_pd[\"date_from\"], df_pd[\"date_to\"] , df_pd[\"user_id\"]\n",
    "            ,df_pd[\"priority\"]\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "mssparkutils.notebook.exit(str(lResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f3791-b96a-4eb4-a9e6-b9821f34c247",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "0089b3bb-3e3f-4921-b047-ba87ec130fb4",
    "default_lakehouse_name": "bronze",
    "default_lakehouse_workspace_id": "2007662d-6654-4a40-be93-23217fe4b693",
    "known_lakehouses": [
     {
      "id": "49f908d1-4437-4cc0-b70d-f2ee89449202"
     },
     {
      "id": "0089b3bb-3e3f-4921-b047-ba87ec130fb4"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
