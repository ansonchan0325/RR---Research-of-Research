{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1622b8cc-71bd-4669-bd22-fd7778ecf1e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "aab48b85-6b43-400a-9afb-e0da12434019",
       "queued_time": "2024-11-12T19:19:58.5350368Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import csv\n",
    "import zlib\n",
    "import pandas as pd\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078c1d95-6765-428e-b39e-abc6888ce051",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T05:42:47.1691853Z",
       "execution_start_time": "2024-11-10T05:42:46.9270931Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d9bdaa3f-b32c-4aee-a799-8cd6ae536758",
       "queued_time": "2024-11-10T05:41:13.0094715Z",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 669cd7cd-0251-4696-a1f4-95f98e0976de, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for pipeline paramaters \n",
    "query_id = \"\"\n",
    "query = \"\"\n",
    "paper_num = 0\n",
    "date_from = \"\"  \n",
    "date_to = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ffe0c3-a945-4d60-9d59-5ae68fccc2ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T05:44:29.0217104Z",
       "execution_start_time": "2024-11-10T05:44:28.7837671Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1c75172f-4205-4cfd-adec-a71f74952c15",
       "queued_time": "2024-11-10T05:44:28.3944604Z",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 669cd7cd-0251-4696-a1f4-95f98e0976de, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, TimestampType\n",
    "\n",
    "# Define schema for flattened research paper data\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('hash_id', LongType(), True),  # Unique hash ID for the paper\n",
    "        StructField('title', StringType(), True),  # Title of the paper\n",
    "        StructField('authors', StringType(), True),  # Authors of the paper as a comma-separated string\n",
    "        StructField('published', TimestampType(), True),  # Publication date of the paper\n",
    "        StructField('summary', StringType(), True),  # Abstract or summary of the paper\n",
    "        StructField('pdf_url', StringType(), True),  # URL to download the paper's PDF\n",
    "        StructField('entry_id', StringType(), True),  # Unique entry ID for the paper (e.g., arXiv ID)\n",
    "        StructField('recommended', LongType(), True),  # Flag indicating if the paper is recommended (1 if recommended, 0 otherwise)\n",
    "        StructField('referenceCount', LongType(), True),  # Number of references in the paper\n",
    "        StructField('citationCount', LongType(), True),  # Number of times the paper has been cited\n",
    "        StructField('references', StringType(), True),  # JSON string of references\n",
    "        StructField('citations', StringType(), True),  # JSON string of citations\n",
    "        StructField('s2FieldsOfStudy', StringType(), True),  # JSON string of fields of study\n",
    "        StructField('tldr', StringType(), True),  # JSON string for TLDR summary\n",
    "        StructField('query_id', StringType(), True)  # Identifier for the query that generated the result\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e05e98-08a8-4db7-95b6-88415557f09a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T05:44:29.7025092Z",
       "execution_start_time": "2024-11-10T05:44:29.468778Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "585b205e-8016-40cb-be01-42d177eed15b",
       "queued_time": "2024-11-10T05:44:29.0780047Z",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 669cd7cd-0251-4696-a1f4-95f98e0976de, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import calendar\n",
    "import arxiv\n",
    "import zlib\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import re\n",
    "import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "class ArxivResearchHelper:\n",
    "    def __init__(self, download_dir=\"downloads\", page_size=10, delay_seconds=3.0, num_retries=3):\n",
    "        self.download_dir = download_dir\n",
    "        if not os.path.exists(self.download_dir):\n",
    "            os.makedirs(self.download_dir)\n",
    "        \n",
    "        # Initialize the arxiv.Client with custom settings\n",
    "        self.client = arxiv.Client(\n",
    "            page_size=page_size,\n",
    "            delay_seconds=delay_seconds,\n",
    "            num_retries=num_retries\n",
    "        )\n",
    "    \n",
    "    def format_paper_id(self, entry_id):\n",
    "        \"\"\"\n",
    "        Format the paper ID to match Semantic Scholar's expected format.\n",
    "\n",
    "        Parameters:\n",
    "        - entry_id (str): The arXiv entry ID.\n",
    "\n",
    "        Returns:\n",
    "        - str: Formatted paper ID.\n",
    "        \"\"\"\n",
    "        arxiv_id = re.sub(r\"v\\d+$\", \"\", entry_id.split(\"/\")[-1])\n",
    "        return f\"ARXIV:{arxiv_id}\"\n",
    "\n",
    "    def search_papers(self, query, max_results=50, date_from=None, date_to=None):\n",
    "        \"\"\"\n",
    "        Search for papers on arXiv with an optional date range.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The search query.\n",
    "        - max_results (int): Maximum number of results to return.\n",
    "        - date_from (str): Start date in 'YYYY-MM-DD' format.\n",
    "        - date_to (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "        Returns:\n",
    "        - List of dictionaries containing paper details.\n",
    "        \"\"\"\n",
    "        # Build the date range query if date_from or date_to is specified\n",
    "        if date_from or date_to:\n",
    "            date_query = \"submittedDate:[\"\n",
    "            \n",
    "            # Handle start date\n",
    "            if date_from:\n",
    "                try:\n",
    "                    date_from_parsed = datetime.strptime(date_from, \"%Y-%m-%d\")\n",
    "                    date_query += date_from_parsed.strftime(\"%Y%m%d\") + \" TO \"\n",
    "                except ValueError:\n",
    "                    raise ValueError(\"Invalid date_from format. Use 'YYYY-MM-DD'.\")\n",
    "            else:\n",
    "                date_query += \"* TO \"\n",
    "                \n",
    "            # Handle end date\n",
    "            if date_to:\n",
    "                try:\n",
    "                    date_to_parsed = datetime.strptime(date_to, \"%Y-%m-%d\")\n",
    "                    last_day = calendar.monthrange(date_to_parsed.year, date_to_parsed.month)[1]\n",
    "                    end_date = date_to_parsed.replace(day=last_day)\n",
    "                    date_query += end_date.strftime(\"%Y%m%d\") + \"]\"\n",
    "                except ValueError:\n",
    "                    raise ValueError(\"Invalid date_to format. Use 'YYYY-MM-DD'.\")\n",
    "            else:\n",
    "                date_query += \"*]\"\n",
    "\n",
    "            # Combine the main query with the date range query\n",
    "            query = f\"({query}) AND {date_query}\"\n",
    "\n",
    "        # Create the search object\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate  # Sort by submission date\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        try:\n",
    "            # Fetch results and store in a list\n",
    "            for result in self.client.results(search):\n",
    "                paper = {\n",
    "                    \"hash_id\": zlib.crc32(bytes(result.entry_id, 'utf-8')),\n",
    "                    \"title\": result.title,\n",
    "                    \"authors\": \", \".join([author.name for author in result.authors]),\n",
    "                    \"published\": result.published,\n",
    "                    \"summary\": result.summary,\n",
    "                    \"pdf_url\": result.pdf_url,\n",
    "                    \"entry_id\": result.entry_id,\n",
    "                    \"recommended\": 0  # Flag as original search result\n",
    "                }\n",
    "                results.append(paper)\n",
    "                \n",
    "                if len(results) >= max_results:\n",
    "                    break  # Stop if we reach the max results limit\n",
    "        except Exception as e:\n",
    "            print(f\"Error while fetching results: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_citation_data(self, papers):\n",
    "        \"\"\"\n",
    "        Enrich papers with citation data from Semantic Scholar.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list of dict): List of paper dictionaries.\n",
    "\n",
    "        Returns:\n",
    "        - List of dictionaries containing enriched paper details.\n",
    "        \"\"\"\n",
    "        def chunk_list(lst, chunk_size):\n",
    "            for i in range(0, len(lst), chunk_size):\n",
    "                yield lst[i:i + chunk_size]\n",
    "\n",
    "        all_papers_with_citations = []\n",
    "\n",
    "        # Process papers in batches\n",
    "        for paper_chunk in chunk_list(papers, 100):\n",
    "            paper_ids = []\n",
    "            for paper in paper_chunk:\n",
    "                if \"arxiv.org\" in paper[\"entry_id\"]:\n",
    "                    paper_id = self.format_paper_id(paper[\"entry_id\"])\n",
    "                    paper_ids.append(paper_id)\n",
    "            \n",
    "            # Make a batch request to Semantic Scholar API for each chunk\n",
    "            response = requests.post(\n",
    "                'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "                params={'fields': 'referenceCount,citationCount,tldr,s2FieldsOfStudy,citations,references'},\n",
    "                json={\"ids\": paper_ids}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                citation_data = response.json()\n",
    "                for paper, data in zip(paper_chunk, citation_data):\n",
    "                    if not data:\n",
    "                        continue\n",
    "                    paper['referenceCount'] = data.get(\"referenceCount\", 0)\n",
    "                    paper['citationCount'] = data.get(\"citationCount\", 0)\n",
    "                    paper[\"references\"] = data.get(\"references\", [])\n",
    "                    s = ''\n",
    "                    for ref in paper[\"references\"]:\n",
    "                        s += ref['title'] + '|'\n",
    "                    paper[\"references\"] = s\n",
    "                    paper[\"citations\"] = data.get(\"citations\", [])\n",
    "                    s = ''\n",
    "                    for cit in paper[\"citations\"]:\n",
    "                        s += cit['title'] + '|'\n",
    "                    paper[\"citations\"] = s\n",
    "                    paper['s2FieldsOfStudy'] = data.get(\"s2FieldsOfStudy\", [])\n",
    "                    s = ''\n",
    "                    for field in paper['s2FieldsOfStudy']:\n",
    "                        s += field['category'] + '|'\n",
    "                    paper['s2FieldsOfStudy'] = s\n",
    "                    paper['tldr'] = data.get(\"tldr\", \"\")\n",
    "                    if paper['tldr']:\n",
    "                        paper['tldr'] = paper['tldr']['text']\n",
    "                all_papers_with_citations.extend(paper_chunk)\n",
    "            else:\n",
    "                print(\"Error fetching citation data:\", response.text)\n",
    "            \n",
    "            # Optional: Delay to avoid hitting rate limits\n",
    "            sleep(1)\n",
    "\n",
    "        return all_papers_with_citations\n",
    "\n",
    "    def download_pdf(self, entry_id):\n",
    "        \"\"\"\n",
    "        Download the PDF of a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - entry_id (str): The arXiv entry ID.\n",
    "\n",
    "        Returns:\n",
    "        - str or None: File path of the downloaded PDF or None if failed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            paper = next(self.client.results(arxiv.Search(id_list=[entry_id])))\n",
    "            pdf_url = paper.pdf_url\n",
    "            title = paper.title.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            pdf_filename = os.path.join(self.download_dir, f\"{title}.pdf\")\n",
    "            \n",
    "            if os.path.exists(pdf_filename):\n",
    "                print(f\"PDF already exists: {pdf_filename}\")\n",
    "                return pdf_filename\n",
    "\n",
    "            print(f\"Downloading PDF: {pdf_url}\")\n",
    "            paper.download_pdf(dirpath=self.download_dir, filename=f\"{title}.pdf\")\n",
    "            return pdf_filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error while downloading PDF: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_recommended_papers(self, papers, num_recommendations=5):\n",
    "        \"\"\"\n",
    "        Fetch recommended papers using Semantic Scholar and align them with arXiv entries.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list of dict): List of paper dictionaries.\n",
    "        - num_recommendations (int): Number of recommendations per paper.\n",
    "\n",
    "        Returns:\n",
    "        - List of dictionaries containing recommended paper details.\n",
    "        \"\"\"\n",
    "        recommended_papers = []\n",
    "        seen_entry_ids = set(paper['entry_id'] for paper in papers)\n",
    "\n",
    "        for paper in papers:\n",
    "            paper_id = self.format_paper_id(paper[\"entry_id\"])\n",
    "            try:\n",
    "                # Get recommended papers from Semantic Scholar\n",
    "                response = requests.get(\n",
    "                    f'https://api.semanticscholar.org/recommendations/v1/papers/forpaper/{paper_id}',\n",
    "                    params={'limit': num_recommendations}\n",
    "                )\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    rec_data = response.json().get('recommendedPapers', [])\n",
    "                    for rec in rec_data:\n",
    "                        rec_title = rec.get(\"title\")\n",
    "                        if not rec_title:\n",
    "                            continue\n",
    "\n",
    "                        # Search for the recommended paper on arXiv\n",
    "                        arxiv_results = self.search_papers(query=f'ti:\"{rec_title}\"', max_results=1)\n",
    "                        if arxiv_results:\n",
    "                            arxiv_paper = arxiv_results[0]\n",
    "                            if arxiv_paper['entry_id'] not in seen_entry_ids:\n",
    "                                arxiv_paper['recommended'] = 1  # Flag as recommended paper\n",
    "                                recommended_papers.append(arxiv_paper)\n",
    "                                seen_entry_ids.add(arxiv_paper['entry_id'])\n",
    "                else:\n",
    "                    print(f\"Error fetching recommendations for {paper_id}: {response.status_code} {response.text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception fetching recommendations for {paper_id}: {e}\")\n",
    "\n",
    "        return recommended_papers\n",
    "\n",
    "    def save_papers_to_csv(self, papers, filename='papers.csv'):\n",
    "        \"\"\"\n",
    "        Save the list of paper dictionaries to a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list of dict): List of paper dictionaries.\n",
    "        - filename (str): Filename for the CSV file.\n",
    "        \"\"\"\n",
    "        if not papers:\n",
    "            print(\"No papers to save.\")\n",
    "            return\n",
    "\n",
    "        keys = papers[0].keys()\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            dict_writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(papers)\n",
    "\n",
    "    def search_papers_aug(self, query, max_results=50, date_from=None, date_to=None):\n",
    "        \"\"\"\n",
    "        Search for papers and include recommended papers.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The search query.\n",
    "        - max_results (int): Maximum number of search results to return.\n",
    "        - date_from (str): Start date in 'YYYY-MM-DD' format.\n",
    "        - date_to (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "        Returns:\n",
    "        - List of dictionaries containing paper details with citation data.\n",
    "        \"\"\"\n",
    "        papers = self.search_papers(query, max_results, date_from, date_to)\n",
    "        # recommended_papers = self.get_recommended_papers(papers, num_recommendations=5)\n",
    "        # all_papers = papers + recommended_papers\n",
    "        papers_with_citations = self.get_citation_data(papers)\n",
    "        return papers_with_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fb50e1-6121-4d7b-bbb2-acbe96a6ca77",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T05:47:18.8356811Z",
       "execution_start_time": "2024-11-10T05:44:30.4752324Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "0ca3f686-08fc-4822-ae48-96d028e896ff",
       "queued_time": "2024-11-10T05:44:30.0732667Z",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 669cd7cd-0251-4696-a1f4-95f98e0976de, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hash_id': 918220639, 'title': 'Data is missing again -- Reconstruction of power generation data using $k$-Nearest Neighbors and spectral graph theory', 'authors': 'Amandine Pierrot, Pierre Pinson', 'published': datetime.datetime(2024, 8, 30, 23, 58, 28, tzinfo=datetime.timezone.utc), 'summary': 'The risk of missing data and subsequent incomplete data records at wind farms\\nincreases with the number of turbines and sensors. We propose here an\\nimputation method that blends data-driven concepts with expert knowledge, by\\nusing the geometry of the wind farm in order to provide better estimates when\\nperforming Nearest Neighbor imputation. Our method relies on learning Laplacian\\neigenmaps out of the graph of the wind farm through spectral graph theory.\\nThese learned representations can be based on the wind farm layout only, or\\nadditionally account for information provided by collected data. The related\\nweighted graph is allowed to change with time and can be tracked in an online\\nfashion. Application to the Westermost Rough offshore wind farm shows\\nsignificant improvement over approaches that do not account for the wind farm\\nlayout information.', 'pdf_url': 'http://arxiv.org/pdf/2409.00300v1', 'entry_id': 'http://arxiv.org/abs/2409.00300v1', 'recommended': 0, 'referenceCount': 31, 'citationCount': 0, 'references': 'Towards Resilient Energy Forecasting: A Robust Optimization Approach|Wind energy forecasting with missing values within a fully conditional specification framework|What\\'s a good imputation to predict with missing values?|Missing data in wind farm time series: Properties and effect on forecasts|A Modern Introduction to Online Learning|Adaptive Confidence Boundary Modeling of Wind Turbine Power Curve Using SCADA Data and Its Application|High‐dimensional principal component analysis with heterogeneous missingness|On the consistency of supervised learning with missing values|BRITS: Bidirectional Recurrent Imputation for Time Series|k*-Nearest Neighbors: From Global to Local|Introduction to Online Convex Optimization|Lectures on the Nearest Neighbor Method|Online Time Series Prediction with Missing Data|Errors and uncertainties associated with missing wind data and short records|Metric Learning: A Survey|What Is Meant by \"Missing at Random\"?|The effect of missing data on wind resource estimation|A tutorial on spectral clustering|Prediction, learning, and games|The Elements of Statistical Learning|Laplacian Eigenmaps for Dimensionality Reduction and Data Representation|Missing value estimation methods for DNA microarrays|Estimation of Time Series Models in the Presence of Missing Data|Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper|INFERENCE AND MISSING DATA|Multivariate Time Series Imputation with Generative Adversarial Networks|Calibration of a Wind Farm Wind Speed Model With Incomplete Wind Data|Matrix Computations, 4th edition|Smooth regression analysis|On Estimating Regression|Appendix Nomenclature η Learning rate of lazy OGD λ Eigenvalue A Adjacency matrix D Degree matrix f Eigenvector L Laplacian matrix z i Embedding of wind turbine i E Set of edges G Graph K|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Mathematics|Engineering|Environmental Science|', 'tldr': None}\n",
      "{'hash_id': 1939709920, 'title': 'On Expressive Power of Quantized Neural Networks under Fixed-Point Arithmetic', 'authors': 'Geonho Hwang, Yeachan Park, Sejun Park', 'published': datetime.datetime(2024, 8, 30, 23, 40, 19, tzinfo=datetime.timezone.utc), 'summary': 'Research into the expressive power of neural networks typically considers\\nreal parameters and operations without rounding error. In this work, we study\\nuniversal approximation property of quantized networks under discrete\\nfixed-point parameters and fixed-point operations that may incur errors due to\\nrounding. We first provide a necessary condition and a sufficient condition on\\nfixed-point arithmetic and activation functions for universal approximation of\\nquantized networks. Then, we show that various popular activation functions\\nsatisfy our sufficient condition, e.g., Sigmoid, ReLU, ELU, SoftPlus, SiLU,\\nMish, and GELU. In other words, networks using those activation functions are\\ncapable of universal approximation. We further show that our necessary\\ncondition and sufficient condition coincide under a mild condition on\\nactivation functions: e.g., for an activation function $\\\\sigma$, there exists a\\nfixed-point number $x$ such that $\\\\sigma(x)=0$. Namely, we find a necessary and\\nsufficient condition for a large class of activation functions. We lastly show\\nthat even quantized networks using binary weights in $\\\\{-1,1\\\\}$ can also\\nuniversally approximate for practical activation functions.', 'pdf_url': 'http://arxiv.org/pdf/2409.00297v1', 'entry_id': 'http://arxiv.org/abs/2409.00297v1', 'recommended': 0, 'referenceCount': 0, 'citationCount': 0, 'references': '', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Mathematics|Computer Science|Mathematics|', 'tldr': 'This work provides a necessary and sufficient condition for a large class of activation functions and shows that even quantized networks using binary weights in $\\\\{-1,1\\\\}$ can also universally approximate for practical activation functions.'}\n",
      "{'hash_id': 1918891479, 'title': 'Credit Scores: Performance and Equity', 'authors': 'Stefania Albanesi, Domonkos F. Vamossy', 'published': datetime.datetime(2024, 8, 30, 23, 36, 2, tzinfo=datetime.timezone.utc), 'summary': 'Credit scores are critical for allocating consumer debt in the United States,\\nyet little evidence is available on their performance. We benchmark a widely\\nused credit score against a machine learning model of consumer default and find\\nsignificant misclassification of borrowers, especially those with low scores.\\nOur model improves predictive accuracy for young, low-income, and minority\\ngroups due to its superior performance with low quality data, resulting in a\\ngain in standing for these populations. Our findings suggest that improving\\ncredit scoring performance could lead to more equitable access to credit.', 'pdf_url': 'http://arxiv.org/pdf/2409.00296v1', 'entry_id': 'http://arxiv.org/abs/2409.00296v1', 'recommended': 0, 'referenceCount': 0, 'citationCount': 0, 'references': '', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Economics|Economics|Business|Computer Science|', 'tldr': 'This work benchmarks a widely used credit score against a machine learning model of consumer default and finds significant misclassification of borrowers, especially those with low scores, suggesting that improving credit scoring performance could lead to more equitable access to credit.'}\n",
      "{'hash_id': 1880705934, 'title': 'Box2Flow: Instance-based Action Flow Graphs from Videos', 'authors': 'Jiatong Li, Kalliopi Basioti, Vladimir Pavlovic', 'published': datetime.datetime(2024, 8, 30, 23, 33, 19, tzinfo=datetime.timezone.utc), 'summary': 'A large amount of procedural videos on the web show how to complete various\\ntasks. These tasks can often be accomplished in different ways and step\\norderings, with some steps able to be performed simultaneously, while others\\nare constrained to be completed in a specific order. Flow graphs can be used to\\nillustrate the step relationships of a task. Current task-based methods try to\\nlearn a single flow graph for all available videos of a specific task. The\\nextracted flow graphs tend to be too abstract, failing to capture detailed step\\ndescriptions. In this work, our aim is to learn accurate and rich flow graphs\\nby extracting them from a single video. We propose Box2Flow, an instance-based\\nmethod to predict a step flow graph from a given procedural video. In detail,\\nwe extract bounding boxes from videos, predict pairwise edge probabilities\\nbetween step pairs, and build the flow graph with a spanning tree algorithm.\\nExperiments on MM-ReS and YouCookII show our method can extract flow graphs\\neffectively.', 'pdf_url': 'http://arxiv.org/pdf/2409.00295v1', 'entry_id': 'http://arxiv.org/abs/2409.00295v1', 'recommended': 0, 'referenceCount': 41, 'citationCount': 0, 'references': 'Dynamic Scene Graph Representation for Surgical Video|Segment Anything|Procedure-Aware Pretraining for Instructional Video Understanding|Multimodal Subtask Graph Generation from Instructional Videos|Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks|VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning|InstructPix2Pix: Learning to Follow Image Editing Instructions|SSGVS: Semantic Scene Graph-to-Video Synthesis|Relation-aware attention for video captioning via graph learning|Graph2Vid: Flow graph to Video Grounding for Weakly-supervised Multi-Step Localization|MIAIS: A Multimedia Recipe Dataset with Ingredient Annotation at Each Instructional Step|SVGraph: Learning Semantic Graphs from Instructional Videos|Object-Relation Reasoning Graph for Action Recognition|Dynamic Scene Graph Generation via Anticipatory Pre-training|Technique for IoT malware detection based on control flow graph analysis|Operation Diagnosis on Procedure Graph: The Task and Dataset|Spatial-Temporal Transformer for Dynamic Scene Graph Generation|SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences|DORi: Discovering Object Relationships for Moment Localization of a Natural Language Query in a Video|Neural Scene Graphs for Dynamic Scenes|Multi-modal Cooking Workflow Construction for Food Recipes|Compositional Video Synthesis with Action Graphs|Improving Action Segmentation via Graph-Based Temporal Reasoning|English Recipe Flow Graph Corpus|MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning|G-TAD: Sub-Graph Localization for Temporal Action Detection|VideoGraph: Recognizing Minutes-Long Human Activities in Videos|Cross-Task Weakly Supervised Learning From Instructional Videos|Parameter-Efficient Transfer Learning for NLP|SlowFast Networks for Video Recognition|Neural Task Graphs: Generalizing to Unseen Tasks From a Single Video Demonstration|End-to-End Dense Video Captioning with Masked Transformer|Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering|Towards Automatic Learning of Procedures From Web Instructional Videos|Microsoft COCO: Common Objects in Context|Flow Graph Corpus from Recipe Texts|A graph distance metric based on the maximal common subgraph|A Hybrid Approach for Detecting Prerequisite Relations in Multi-Modal Food Recipes|Structure-Aware Procedural Text Generation From an Image Sequence|Detectron2|Box2Flow: Instance-based Action Flow Graphs from Videos 15|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|', 'tldr': 'Box2Flow is proposed, an instance-based method to predict a step flow graph from a given procedural video, and can extract bounding boxes from videos, predict pairwise edge probabilities between step pairs, and build the flow graph with a spanning tree algorithm.'}\n",
      "{'hash_id': 1910186425, 'title': 'Quantum Machine Learning for Anomaly Detection in Consumer Electronics', 'authors': 'Sounak Bhowmik, Himanshu Thapliyal', 'published': datetime.datetime(2024, 8, 30, 23, 28, tzinfo=datetime.timezone.utc), 'summary': 'Anomaly detection is a crucial task in cyber security. Technological\\nadvancement brings new cyber-physical threats like network intrusion, financial\\nfraud, identity theft, and property invasion. In the rapidly changing world,\\nwith frequently emerging new types of anomalies, classical machine learning\\nmodels are insufficient to prevent all the threats. Quantum Machine Learning\\n(QML) is emerging as a powerful computational tool that can detect anomalies\\nmore efficiently. In this work, we have introduced QML and its applications for\\nanomaly detection in consumer electronics. We have shown a generic framework\\nfor applying QML algorithms in anomaly detection tasks. We have also briefly\\ndiscussed popular supervised, unsupervised, and reinforcement learning-based\\nQML algorithms and included five case studies of recent works to show their\\napplications in anomaly detection in the consumer electronics field.', 'pdf_url': 'http://arxiv.org/pdf/2409.00294v1', 'entry_id': 'http://arxiv.org/abs/2409.00294v1', 'recommended': 0, 'referenceCount': 23, 'citationCount': 0, 'references': 'Quantum Machine Learning for Security Assessment in the Internet of Medical Things (IoMT)|Detection of anomaly in surveillance videos using quantum convolutional neural networks|A Quantum-Classical Hybrid Solution for Deep Anomaly Detection|Hybrid Quantum-Classical Neural Networks|Unsupervised quantum machine learning for fraud detection|Hybrid classical-quantum autoencoder for anomaly detection|The Variational Quantum Eigensolver: A review of methods and best practices|Quantum machine learning models are kernel methods|Design of Quantum Computing Circuits|Supervised learning with quantum-enhanced feature spaces|Quantum Computing Circuits and Devices|Circuit-centric quantum classifiers|Quantum machine learning|Quantum algorithms for supervised and unsupervised machine learning|Quantum Computation and Quantum Information|Quantum Reinforcement Learning|Quantum Mechanics Helps in Searching for a Needle in a Haystack|Scheme for reducing decoherence in quantum computer memory.|LSTM based Anomaly Detection of PFCP Signaling Attacks in 5G Networks|Quantum Machine Learning Applications in the Biomedical Domain: A Systematic Review|Quantum Algorithm for Ensemble Learning|“Quantum support vector machine for big data classification,”|Fault-tolerant Quantum Computation|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Physics|Computer Science|Engineering|', 'tldr': 'A generic framework for applying QML algorithms in anomaly detection tasks is shown and popular supervised, unsupervised, and reinforcement learning-based QML algorithms are discussed.'}\n",
      "{'hash_id': 1378396479, 'title': 'Benchmarking the Performance of Large Language Models on the Cerebras Wafer Scale Engine', 'authors': 'Zuoning Zhang, Dhruv Parikh, Youning Zhang, Viktor Prasanna', 'published': datetime.datetime(2024, 8, 30, 22, 45, 49, tzinfo=datetime.timezone.utc), 'summary': \"Transformer based Large Language Models (LLMs) have recently reached state of\\nthe art performance in Natural Language Processing (NLP) and Computer Vision\\n(CV) domains. LLMs use the Multi-Headed Self-Attention (MHSA) mechanism to\\ncapture long-range global attention relationships among input words or image\\npatches, drastically improving its performance over prior deep learning\\napproaches. In this paper, we evaluate the performance of LLMs on the Cerebras\\nWafer Scale Engine (WSE). Cerebras WSE is a high performance computing system\\nwith 2.6 trillion transistors, 850,000 cores and 40 GB on-chip memory. Cerebras\\nWSE's Sparse Linear Algebra Compute (SLAC) cores eliminates multiply-by-zeros\\noperations and its 40 GB of on-chip memory is uniformly distributed among SLAC\\ncores, enabling fast local access to model parameters. Moreover, Cerebras\\nsoftware configures routing between cores at runtime, optimizing communication\\noverhead among cores. As LLMs are becoming more commonly used, new hardware\\narchitectures are needed to accelerate LLMs training and inference. We\\nbenchmark the effectiveness of this hardware architecture at accelerating LLMs\\ntraining and inference. Additionally, we analyze if Cerebras WSE can scale the\\nmemory-wall associated with traditionally memory-bound compute tasks using its\\n20 PB/s high bandwidth memory. Furthermore, we examine the performance\\nscalability of Cerebras WSE through a roofline model. By plotting performance\\nmetrics against computational intensity, we aim to assess their effectiveness\\nat handling high compute-intensive LLMs training and inference tasks.\", 'pdf_url': 'http://arxiv.org/pdf/2409.00287v2', 'entry_id': 'http://arxiv.org/abs/2409.00287v2', 'recommended': 0, 'referenceCount': 27, 'citationCount': 0, 'references': 'Simulating Classroom Education with LLM-Empowered Agents|New Solutions on LLM Acceleration, Optimization, and Application|Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference|LLM-Based Framework for Administrative Task Automation in Healthcare|MediSwift: Efficient Sparse Pre-trained Biomedical Language Models|LLM-Based Code Generation Method for Golang Compiler Testing|Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models|Cerebras Architecture Deep Dive: First Look Inside the Hardware/Software Co-Design for Deep Learning|ChatGPT: Fundamentals, Applications and Social Impacts|Recent Progress in Conversational AI|Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space|Datasheet for the Pile|The Path to Successful Wafer-Scale Integration: The Cerebras Story|The Pile: An 800GB Dataset of Diverse Text for Language Modeling|Language Models are Few-Shot Learners|BERT for Stock Market Sentiment Analysis|Masked Language Model Scoring|Bilingual Machine Translation Using RNN Based Deep Learning|Recent Advances in Recurrent Neural Networks|Learned in Translation: Contextualized Word Vectors|Attention is All you Need|EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding|Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank|Finding Structure in Time|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|Long short-term memory recurrent neural network architectures for large scale acoustic modeling|“Chatgpt: Optimizing language models for dialogue|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|', 'tldr': 'This paper benchmarks the effectiveness of this hardware architecture at accelerating LLMs training and inference on the Cerebras WSE, and examines the performance scalability of Cerebras WSE through a roofline model.'}\n",
      "{'hash_id': 1349392230, 'title': 'Reframing Data Value for Large Language Models Through the Lens of Plausibility', 'authors': 'Mohamad Rida Rammal, Ruida Zhou, Suhas Diggavi', 'published': datetime.datetime(2024, 8, 30, 22, 32, 24, tzinfo=datetime.timezone.utc), 'summary': 'Data valuation seeks to answer the important question, \"How much is this data\\nworth?\" Existing data valuation methods have largely focused on discriminative\\nmodels, primarily examining data value through the lens of its utility in\\ntraining. However, with the push for ever-larger language models, relying on\\nvaluation methods that require training becomes increasingly expensive and\\ndependent on specific techniques. We propose an alternative perspective on the\\ndata value problem for language models, centering around the plausibility of\\nthe data. We posit that data holds lesser value if it can be plausibly\\ngenerated by the model itself. Starting from some intuitive criteria that align\\nwith our notions of valuable data, we develop a novel value function that is\\ncomputationally tractable and derived from first principles with provable\\nproperties. We conduct a theoretical analysis of our value function and\\nevaluate it across multiple scenarios and datasets.', 'pdf_url': 'http://arxiv.org/pdf/2409.00284v2', 'entry_id': 'http://arxiv.org/abs/2409.00284v2', 'recommended': 0, 'referenceCount': 23, 'citationCount': 0, 'references': 'LAVA: Data Valuation without Pre-Specified Learning Algorithms|Data Valuation Without Training of a Model|The Price of Tolerance in Distribution Testing|Characterizing Structural Regularities of Labeled Data in Overparameterized Models|Data Shapley: Equitable Valuation of Data for Machine Learning|Towards Efficient Data Valuation Based on the Shapley Value|Minimax Testing of Identity to a Reference Ergodic Markov Chain|Sample-Optimal Identity Testing with High Probability|Optimal Testing for Properties of Distributions|Asymptotic minimax regret for data compression, gambling, and prediction|The Tight Constant in the Dvoretzky-Kiefer-Wolfowitz Inequality|Asymptotic Minimax Character of the Sample Distribution Function and of the Classical Multinomial Estimator|Remarks on a Multivariate Transformation|DAVINZ: Data Valuation using Deep Neural Networks at Initialization|Validation Free and Replication Robust Volume-based Data Valuation|A Mathematical Theory of Communication|Cover and|The art of computer programming, volume 2 (3rd ed.): seminumerical algorithms|Chapter 3 - learning and identity testing of markov chains|Similarity-based data|we can reduce each of the above sampling method to multinomial sampling methods with a modified probability distribution|Information theory: From coding to learning|Lossless text compression using large language models|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|Linguistics|', 'tldr': 'This work proposes an alternative perspective on the data value problem for language models, centering around the plausibility of the data, and posit that data holds lesser value if it can be plausibly generated by the model itself.'}\n",
      "{'hash_id': 18349652, 'title': 'Prediction of excitable wave dynamics using machine learning', 'authors': 'Mahesh Kumar Mulimani, Sebastian Echeverria-Alar, Michael Reiss, Wouter-Jan Rappel', 'published': datetime.datetime(2024, 8, 30, 22, 20, 29, tzinfo=datetime.timezone.utc), 'summary': 'Excitable systems can exhibit a variety of dynamics with different\\ncomplexity, ranging from a single, stable spiral to spiral defect chaos (SDC),\\nduring which spiral waves are continuously formed and destroyed. The\\ncorresponding reaction-diffusion models, including ones for cardiac tissue, can\\ninvolve a large number of variables and can be time-consuming to simulate. Here\\nwe trained a deep-learning (DL) model using snapshots from a single variable,\\nobtained by simulating a single quasi-periodic spiral wave and SDC using a\\ngeneric cardiac model. Using the trained DL model, we predicted the dynamics in\\nboth cases, using timesteps that are much larger than required for the\\nsimulations of the underlying equations. We show that the DL model is able to\\npredict the trajectory of a quasi-periodic spiral wave and that the SDC\\nactivaton patterns can be predicted for approximately one Lyapunov time.\\nFurthermore, we show that the DL model accurately captures the statistics of\\ntermination events in SDC, including the mean termination time. Finally, we\\nshow that a DL model trained using a specific domain size is able to replicate\\ntermination statistics on larger domains, resulting in significant\\ncomputational savings.', 'pdf_url': 'http://arxiv.org/pdf/2409.00278v2', 'entry_id': 'http://arxiv.org/abs/2409.00278v2', 'recommended': 0, 'referenceCount': 7, 'citationCount': 0, 'references': 'Adam: A Method for Stochastic Optimization|Simulation of the Undiseased Human Cardiac Ventricular Action Potential: Model Formulation and Experimental Validation|The Lyapunov Characteristic Exponents and Their Computation|Alternans and spiral breakup in a human ventricular tissue model.|Vortex dynamics in three-dimensional continuous myocardium with fiber rotation: Filament instability and fibrillation.|A simple two-variable model of cardiac excitation|AN INTERDISCIPLINARY|', 'citations': '', 's2FieldsOfStudy': 'Physics|Medicine|Engineering|Computer Science|', 'tldr': 'A deep-learning model trained using snapshots from a single variable is able to predict the trajectory of a quasi-periodic spiral wave and that the SDC activaton patterns can be predicted for approximately one Lyapunov time, and it is shown that the DL model accurately captures the statistics of termination events in SDC, including the mean termination time.'}\n",
      "{'hash_id': 2470635731, 'title': 'Analysis of Status Update in Wireless Networks with Successive Interference Cancellation', 'authors': 'Asmad Bin Abdul Razzaque, Andrea Baiocchi', 'published': datetime.datetime(2024, 8, 30, 22, 16, 18, tzinfo=datetime.timezone.utc), 'summary': 'Data collection in an IoT environment requires simple and effective\\ncommunication solutions to address resource constraints, ensure network\\nefficiency, while achieving scalability. Efficiency is evaluated based on the\\ntimeliness of collected data (Age of Information), the energy spent per\\ndelivered unit of data, and the effectiveness in utilizing spectrum resources.\\nThis paper addresses a random multiple access adaptive system, in which a large\\nnumber of devices send sporadic messages in non-periodic pattern. In\\nparticular, our analysis highlights the potential of Successive Interference\\nCancellation and identifies an adaptive parameter setting to maximize its\\nbenefits as the level of contention on the shared channel varies. An analytical\\nmodel is defined, easily scalable with the number of nodes and yielding all the\\nrelevant metrics. Evidence of the accuracy of the model is given by comparing\\npredicted results against simulations. The model is utilized to assess the\\ntrade-off between Age of Information and energy consumption, revealing a sharp\\nrelationship between the two. The considered approach lends itself to many\\ngeneralizations and applications to massive machine-type communications and IoT\\nnetworks.', 'pdf_url': 'http://arxiv.org/pdf/2409.00277v1', 'entry_id': 'http://arxiv.org/abs/2409.00277v1', 'recommended': 0, 'referenceCount': 50, 'citationCount': 0, 'references': 'Asymptotic analysis of sum-rate under SIC|Age of Information Minimization for Frameless ALOHA in Grant-Free Massive Access|NOMA-Based Grant-Free Massive Access for Latency-Critical Internet of Things: A Scalable and Reliable Framework|Novel Access Methods in Cellular Networks Based on the Analog Bloom Filter|NOMA-Assisted Grant-Free Transmission: How to Design Pre-Configured SNR Levels?|Low vs high spectral efficiency communications with SIC and random access|RACH Success Probability Analysis and Optimization in NB-IoT Networks|Deep Learning Based Successive Interference Cancellation for the Non-Orthogonal Downlink|To Buffer or Not To Buffer: IEEE 802.11p/bd Performance Under Different Buffering Strategies|Machine type communications: key drivers and enablers towards the 6G era|A Survey on Successive Interference Cancellation Schemes in Non-Orthogonal Multiple Access for Future Radio Access|Age of Information in Grant-Free Random Access With Massive MIMO|Grant-Free Random Access in Machine-Type Communication: Approaches and Challenges|Deep Learning-aided Successive Interference Cancellation for MIMO-NOMA|Age of Information: An Introduction and Survey|Andrea|Overcoming 5G PRACH Capacity Shortfall: Supersets of Zadoff–Chu Sequences With Low-Correlation Zone|White Paper on Critical and Massive Machine Type Communication Towards 6G|Performance Analysis of Grant-Free Multiple Access for Supporting Sporadic Traffic in Massive IoT Networks|Grant-Free Non-Orthogonal Multiple Access for IoT: A Survey|On the Design of Massive Non-Orthogonal Multiple Access With Imperfect Successive Interference Cancellation|Sum-Rate Maximization of NOMA Systems Under Imperfect Successive Interference Cancellation|On the Role of Age of Information in the Internet of Things|Toward Massive Machine Type Communications in Ultra-Dense Cellular IoT Networks: Current Issues and Machine Learning-Assisted Solutions|Performance Analysis and Optimal Access Class Barring Parameter Configuration in LTE-A Networks With Massive M2M Traffic|Nonorthogonal Multiple Access for 5G and Beyond|Modeling the Energy Performance of LoRaWAN|Non-Orthogonal Multiple Access for Large-Scale 5G Networks: Interference Aware Design|Ultra-Reliable Low Latency Cellular Networks: Use Cases, Challenges and Approaches|Measurement-Based Signaling Management Strategies for Cellular IoT|NOMA in 5G Systems: Exciting Possibilities for Enhancing Spectral Efficiency|Non-Orthogonal Multiple Access in Large-Scale Heterogeneous Networks|Massive Non-Orthogonal Multiple Access for Cellular IoT: Potentials and Limitations|Impact of User Pairing on 5G Nonorthogonal Multiple-Access Downlink Transmissions|Massive machine-type communications in 5g: physical and MAC-layer solutions|Non-orthogonal multiple access (NOMA): Concept, performance evaluation and experimental trials|Non-orthogonal Multiple Access (NOMA) with Successive Interference Cancellation for Future Radio Access|Successive Interference Cancellation in Heterogeneous Networks|Successive Interference Cancellation: Carving Out MAC Layer Opportunities|A Survey on the Successive Interference Cancellation Performance for Single-Antenna and Multiple-Antenna OFDM Systems|The Performance of Successive Interference Cancellation in Random Wireless Networks|On the applicability of Two-Ray path loss models for vehicular network simulation|Real-time status: How often should one update?|Award|Multipacket reception in random access wireless networks: from signal processing to optimal medium access control|URLLC in Beyond 5G and 6G Networks: An Interference Management Perspective|Performance Analysis of Grant-Free Random-Access NOMA in URLL IoT Networks|has participated Program Committees of 80 international conferences|(UET) and the M.Sc. degree in Electrical Engineering with a from the National University of Sciences and Currently, he is pursuing a Ph.D|[Telecom Italia (currently TIM)] for ten years|', 'citations': '', 's2FieldsOfStudy': 'Engineering|Computer Science|Computer Science|Engineering|', 'tldr': 'This paper addresses a random multiple access adaptive system, in which a large number of devices send sporadic messages in non-periodic pattern, and highlights the potential of Successive Interference Cancellation and identifies an adaptive parameter setting to maximize its benefits as the level of contention on the shared channel varies.'}\n",
      "{'hash_id': 193581918, 'title': 'Exact Recovery Guarantees for Parameterized Non-linear System Identification Problem under Adversarial Attacks', 'authors': 'Haixiang Zhang, Baturalp Yalcin, Javad Lavaei, Eduardo D. Sontag', 'published': datetime.datetime(2024, 8, 30, 22, 12, 57, tzinfo=datetime.timezone.utc), 'summary': 'In this work, we study the system identification problem for parameterized\\nnon-linear systems using basis functions under adversarial attacks. Motivated\\nby the LASSO-type estimators, we analyze the exact recovery property of a\\nnon-smooth estimator, which is generated by solving an embedded $\\\\ell_1$-loss\\nminimization problem. First, we derive necessary and sufficient conditions for\\nthe well-specifiedness of the estimator and the uniqueness of global solutions\\nto the underlying optimization problem. Next, we provide exact recovery\\nguarantees for the estimator under two different scenarios of boundedness and\\nLipschitz continuity of the basis functions. The non-asymptotic exact recovery\\nis guaranteed with high probability, even when there are more severely\\ncorrupted data than clean data. Finally, we numerically illustrate the validity\\nof our theory. This is the first study on the sample complexity analysis of a\\nnon-smooth estimator for the non-linear system identification problem.', 'pdf_url': 'http://arxiv.org/pdf/2409.00276v2', 'entry_id': 'http://arxiv.org/abs/2409.00276v2', 'recommended': 0, 'referenceCount': 35, 'citationCount': 1, 'references': 'Exact Recovery for System Identification with More Corrupt Data than Clean Data|Learning of Dynamical Systems under Adversarial Attacks - Null Space Property Perspective|Statistical Learning Theory for Control: A Finite-Sample Perspective|Single Trajectory Nonparametric Learning of Nonlinear Dynamics|Learning of Dynamical Systems under Adversarial Attacks|Policy Smoothing for Provably Robust Reinforcement Learning|CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing|High-Dimensional Probability: An Introduction with Applications in Data Science|High‐dimensional Statistics: A Non‐asymptotic Viewpoint, Martin J.Wainwright, Cambridge University Press, 2019, xvii 552 pages, £57.99, hardback ISBN: 978‐1‐1084‐9802‐9|Learning nonlinear dynamical systems from a single trajectory|Non-asymptotic and Accurate Learning of Nonlinear Dynamical Systems|Bridging Convex and Nonconvex Optimization in Robust PCA: Noise, Outliers, and Missing Data|Finite Sample Analysis of Stochastic System Identification|Safely Learning to Control the Constrained Linear Quadratic Regulator|Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification|Finite Time Identification in Unstable Linear Systems|On the Sample Complexity of the Linear Quadratic Regulator|On a Class of Optimization-Based Robust Estimators|Characterization of the equivalence of robustification and regularization in linear and matrix regression|Nonlinear system identification in structural dynamics: 10 more years of progress|Analysis of a nonsmooth optimization approach to robust estimation|Linear Regression Analysis|Robust principal component analysis?|Robustness and Regularization of Support Vector Machines|Stability Bounds for Non-i.i.d. Processes|Consistency and asymptotic normality of some subspace algorithms for systems without observed inputs|Asymptotic properties of the least-squares method for estimating transfer functions and disturbance spectra|Identification and Stochastic Adaptive Control|Generalization bounds for non-stationary mixing processes|Robust Regression|CVX: Matlab software for disciplined convex programming, version 2.1|Regret bounds for the adaptive control of linear quadratic systems|Wiley Encyclopedia of Electrical and Electronics Engineering|Nonlinear System Identification|Model-based re-inforcement learning: A survey|', 'citations': 'Prevailing against Adversarial Noncentral Disturbances: Exact Recovery of Linear Systems with the $l_1$-norm Estimator|', 's2FieldsOfStudy': 'Computer Science|Mathematics|Engineering|Computer Science|Engineering|Mathematics|', 'tldr': 'This work analyzes the exact recovery property of a non-smooth estimator, which is generated by solving an embedded $\\\\ell_1$-loss minimization problem, and derives necessary and sufficient conditions for the well-specifiedness of the estimator and the uniqueness of global solutions to the underlying optimization problem.'}\n",
      "{'hash_id': 3801494915, 'title': 'A multiscale method for data collected from network edges via the line graph', 'authors': 'Dingjia Cao, Marina I. Knight, Guy P. Nason', 'published': datetime.datetime(2024, 10, 17, 15, 56, 55, tzinfo=datetime.timezone.utc), 'summary': \"Data collected over networks can be modelled as noisy observations of an\\nunknown function over the nodes of a graph or network structure, fully\\ndescribed by its nodes and their connections, the edges. In this context,\\nfunction estimation has been proposed in the literature and typically makes use\\nof the network topology such as relative node arrangement, often using given or\\nartificially constructed node Euclidean coordinates. However, networks that\\narise in fields such as hydrology (for example, river networks) present\\nfeatures that challenge these established modelling setups since the target\\nfunction may naturally live on edges (e.g., river flow) and/or the\\nnode-oriented modelling uses noisy edge data as weights. This work tackles\\nthese challenges and develops a novel lifting scheme along with its associated\\n(second) generation wavelets that permit data decomposition across the network\\nedges. The transform, which we refer to under the acronym LG-LOCAAT, makes use\\nof a line graph construction that first maps the data in the line graph domain.\\nWe thoroughly investigate the proposed algorithm's properties and illustrate\\nits performance versus existing methodologies. We conclude with an application\\npertaining to hydrology that involves the denoising of a water quality index\\nover the England river network, backed up by a simulation study for a river\\nflow dataset.\", 'pdf_url': 'http://arxiv.org/pdf/2410.13693v1', 'entry_id': 'http://arxiv.org/abs/2410.13693v1', 'recommended': 1, 'referenceCount': 46, 'citationCount': 0, 'references': 'The GNAR-edge model: a network autoregressive model for networks with time-varying edge weights|Complex-network-based traffic network analysis and dynamics: A comprehensive review|Lifting scheme for streamflow data in\\xa0river networks|Non‐parametric regression for networks|Generalised Network Autoregressive Processes and the GNAR package|Manifolds|Hydrological data uncertainty and its implications|Estimation of Subgraph Densities in Noisy Networks|Flow‐directed PCA for monitoring networks|Vertex-Frequency Analysis on Graphs|Networks and the Epidemiology of Infectious Disease|Multiscale Wavelets on Trees, Graphs and High Dimensional Data: Theory and Applications to Semi Supervised Learning|A ‘nondecimated’ lifting transform|Complex brain networks: graph theoretical analysis of structural and functional systems|Multiscale methods for data on graphs and irregular multidimensional situations|High-Order Regularization on Graphs|Modelling stream fish species distribution in a river network: the relative effects of temperature versus physical factors|Spatial statistical models that use flow and stream distance|From graph to manifold Laplacian: The convergence rate|Adaptive lifting for nonparametric regression|Spatial prediction on a river network|Needles and straw in haystacks: Empirical Bayes estimates of possibly sparse sequences|Structural analysis of network traffic flows|Quantum graphs: I. Some basic structures|The Evolution of Social and Economic Networks|HOW IS A GRAPH LIKE A MANIFOLD|A Stabilized Lifting Construction of Wavelets on Irregular Meshes on the Interval|The lifting scheme: a construction of second generation wavelets|CART AND BEST-ORTHO-BASIS: A CONNECTION\\'|The Lifting Scheme: A Custom - Design Construction of Biorthogonal Wavelets \"Industrial Mathematics|Understanding and using genetic algorithms Part 2. Representation, configuration and hybridization|Ideal spatial adaptation by wavelet shrinkage|Understanding and using genetic algorithms Part 1. Concepts, properties and context|Biorthogonal bases of compactly supported wavelets|Vertex-frequency graph signal processing: A comprehensive review|Modern Graph Theory vol. 184|Multiscale, multi-dimensional space and space-time function estimation for irregular network data|Wavelet Methods in Statistics with R|Statistical Modeling by Wavelets vol. 503|Smoothing Non-equidistantly Sampled Data Using Wavelets and Cross Validation|Metrized Graphs, Laplacian Operators, and Electrical Networks|The structure of claw-free graphs|Second generation wavelets and applications|Advanced Linear Algebra vol. 3|accuracy and stability of numerical algorithms|Graph Theory with Applications vol. 290|', 'citations': '', 's2FieldsOfStudy': 'Mathematics|Environmental Science|Computer Science|Engineering|', 'tldr': 'This work develops a novel lifting scheme along with its associated (second) generation wavelets that permit data decomposition across the network edges and develops an application pertaining to hydrology that involves the denoising of a water quality index over the England river network.'}\n",
      "{'hash_id': 1175210199, 'title': 'Boosting K-means for Big Data by Fusing Data Streaming with Global Optimization', 'authors': 'Ravil Mussabayev, Rustam Mussabayev', 'published': datetime.datetime(2024, 10, 18, 15, 43, 34, tzinfo=datetime.timezone.utc), 'summary': 'K-means clustering is a cornerstone of data mining, but its efficiency\\ndeteriorates when confronted with massive datasets. To address this limitation,\\nwe propose a novel heuristic algorithm that leverages the Variable Neighborhood\\nSearch (VNS) metaheuristic to optimize K-means clustering for big data. Our\\napproach is based on the sequential optimization of the partial objective\\nfunction landscapes obtained by restricting the Minimum Sum-of-Squares\\nClustering (MSSC) formulation to random samples from the original big dataset.\\nWithin each landscape, systematically expanding neighborhoods of the currently\\nbest (incumbent) solution are explored by reinitializing all degenerate and a\\nvarying number of additional centroids. Extensive and rigorous experimentation\\non a large number of real-world datasets reveals that by transforming the\\ntraditional local search into a global one, our algorithm significantly\\nenhances the accuracy and efficiency of K-means clustering in big data\\nenvironments, becoming the new state of the art in the field.', 'pdf_url': 'http://arxiv.org/pdf/2410.14548v1', 'entry_id': 'http://arxiv.org/abs/2410.14548v1', 'recommended': 1, 'referenceCount': 15, 'citationCount': 0, 'references': 'Variable Landscape Search: A Novel Metaheuristic Paradigm for Unlocking Hidden Dimensions in Global Optimization|High-Performance Hybrid Algorithm for Minimum Sum-of-Squares Clustering of Infinitely Tall Data|Constructing the Neighborhood Structure of VNS Based on Binomial Distribution for Solving QUBO Problems|How to Use K-means for Big Data Clustering?|Clustering in large data sets with the limited memory bundle method|Qualitative properties of the minimum sum-of-squares clustering problem|HG-means: A scalable hybrid genetic algorithm for minimum sum-of-squares clustering|Python accelerators for high-performance computing|Variable neighborhood search: basics and variants|Variable neighborhood search|NP-hardness of Euclidean sum-of-squares clustering|Heuristic and Meta-Heuristic Algorithms and Their Relevance to the Real World: A Survey|The neighborhood change step follows Algorithm 3|Fact 1: A solution that constitutes a local optimum within one neighborhood structure does not necessarily remain optimal when analyzed under a di ff erent neighborhood|Fact 2: A solution that achieves global optimality will simultaneously be a local optimum across all conceivable neighborhood structures|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Mathematics|Computer Science|Mathematics|', 'tldr': 'A novel heuristic algorithm that leverages the Variable Neighborhood Search (VNS) metaheuristic to optimize K-means clustering for big data and significantly enhances the accuracy and efficiency of K-means clustering in big data environments, becoming the new state of the art in the field.'}\n",
      "{'hash_id': 2292658129, 'title': 'K-Means Clustering With Incomplete Data with the Use of Mahalanobis Distances', 'authors': 'Lovis Kwasi Armah, Igor Melnykov', 'published': datetime.datetime(2024, 10, 31, 0, 5, 9, tzinfo=datetime.timezone.utc), 'summary': 'Effectively applying the K-means algorithm to data with missing values\\nremains an important research area due to its impact on applications that rely\\non K-means clustering. Recent studies have shown that integrating imputation\\ndirectly into the K-means algorithm yields superior results compared to\\nhandling imputation separately.\\n  In this work, we extend this approach by developing a unified K-means\\nalgorithm that incorporates Mahalanobis distances, instead of the traditional\\nEuclidean distances, which previous research has shown to perform better for\\nclusters with elliptical shapes.\\n  We conduct extensive experiments on synthetic datasets containing up to ten\\nelliptical clusters, as well as the IRIS dataset. Using the Adjusted Rand Index\\n(ARI) and Normalized Mutual Information (NMI), we demonstrate that our\\nalgorithm consistently outperforms both standalone imputation followed by\\nK-means (using either Mahalanobis or Euclidean distance) and recent K-means\\nalgorithms that integrate imputation and clustering for handling incomplete\\ndata. These results hold across both the IRIS dataset and randomly generated\\ndata with elliptical clusters.', 'pdf_url': 'http://arxiv.org/pdf/2411.00870v1', 'entry_id': 'http://arxiv.org/abs/2411.00870v1', 'recommended': 1, 'referenceCount': 12, 'citationCount': 0, 'references': 'The k-means Algorithm: A Comprehensive Survey and Performance Evaluation|K-Means Clustering With Incomplete Data|On comparing partitions|A new algorithm for initial cluster centers in k-means algorithm|Missing value estimation methods for DNA microarrays|An empirical comparison of four initialization methods for the K-Means algorithm|Data clustering: a review|Statistical Analysis With Missing Data|Multiple Imputation for Nonresponse in Surveys|Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper|On K-means algorithm with the use of Mahalanobis distances|Cluster Ensembles --- A Knowledge Reuse Framework for Combining Multiple Partitions|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|Mathematics|', 'tldr': 'This work develops a unified K-means algorithm that incorporates Mahalanobis distances, instead of the traditional Euclidean distances, which previous research has shown to perform better for clusters with elliptical shapes.'}\n",
      "{'hash_id': 4130591113, 'title': 'Numerical Approximation Capacity of Neural Networks with Bounded Parameters: Do Limits Exist, and How Can They Be Measured?', 'authors': 'Li Liu, Tengchao Yu, Heng Yong', 'published': datetime.datetime(2024, 9, 25, 7, 43, 48, tzinfo=datetime.timezone.utc), 'summary': 'The Universal Approximation Theorem posits that neural networks can\\ntheoretically possess unlimited approximation capacity with a suitable\\nactivation function and a freely chosen or trained set of parameters. However,\\na more practical scenario arises when these neural parameters, especially the\\nnonlinear weights and biases, are bounded. This leads us to question:\\n\\\\textbf{Does the approximation capacity of a neural network remain universal,\\nor does it have a limit when the parameters are practically bounded? And if it\\nhas a limit, how can it be measured?}\\n  Our theoretical study indicates that while universal approximation is\\ntheoretically feasible, in practical numerical scenarios, Deep Neural Networks\\n(DNNs) with any analytic activation functions (such as Tanh and Sigmoid) can\\nonly be approximated by a finite-dimensional vector space under a bounded\\nnonlinear parameter space (NP space), whether in a continuous or discrete\\nsense. Based on this study, we introduce the concepts of \\\\textit{$\\\\epsilon$\\nouter measure} and \\\\textit{Numerical Span Dimension (NSdim)} to quantify the\\napproximation capacity limit of a family of networks both theoretically and\\npractically.\\n  Furthermore, drawing on our new theoretical study and adopting a fresh\\nperspective, we strive to understand the relationship between back-propagation\\nneural networks and random parameter networks (such as the Extreme Learning\\nMachine (ELM)) with both finite and infinite width. We also aim to provide\\nfresh insights into regularization, the trade-off between width and depth,\\nparameter space, width redundancy, condensation, and other related important\\nissues.', 'pdf_url': 'http://arxiv.org/pdf/2409.16697v1', 'entry_id': 'http://arxiv.org/abs/2409.16697v1', 'recommended': 1, 'referenceCount': 20, 'citationCount': 0, 'references': 'Implicit Regularization of Dropout|Towards Understanding the Condensation of Neural Networks at Initial Training|Approximation capability of two hidden layer feedforward neural networks with fixed weights|On the Approximation by Single Hidden Layer Feed-forward Neural Networks With Fixed Weights|Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations|Measure Theoretic Results for Approximation by Neural Networks with Limited Weights|Approximation by ridge functions and neural networks with a bounded number of neurons|Approximation by neural networks with weights varying on a finite set of directions|A study on effectiveness of extreme learning machine|Extreme learning machine: Theory and applications|An approximation by neural networkswith a fixed weight|Lower bounds for approximation by MLP neural networks|Approximation theory of the MLP model in neural networks|Approximation by ridge functions and neural networks with one hidden layer|Original Contribution: Approximation of continuous functions on Rd by linear combinations of shifted rotations of a sigmoid function with and without scaling|Approximating and learning unknown mappings using multilayer feedforward networks with bounded weights|Approximation by superpositions of a sigmoidal function|Multilayer feedforward networks are universal approximators|the width naturally has redundancy|discussions about depth and width are primarily based on empirical studies|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|Mathematics|', 'tldr': 'The concepts of Epsilon outer measure and NSdim are introduced to quantify the approximation capacity limit of a family of networks both theoretically and practically to understand the relationship between back-propagation neural networks and random parameter networks with both finite and infinite width.'}\n",
      "{'hash_id': 2109940655, 'title': 'Neural Networks Generalize on Low Complexity Data', 'authors': 'Sourav Chatterjee, Timothy Sudijono', 'published': datetime.datetime(2024, 9, 19, 3, 54, 49, tzinfo=datetime.timezone.utc), 'summary': 'We show that feedforward neural networks with ReLU activation generalize on\\nlow complexity data, suitably defined. Given i.i.d. data generated from a\\nsimple programming language, the minimum description length (MDL) feedforward\\nneural network which interpolates the data generalizes with high probability.\\nWe define this simple programming language, along with a notion of description\\nlength of such networks. We provide several examples on basic computational\\ntasks, such as checking primality of a natural number, and more. For primality\\ntesting, our theorem shows the following. Suppose that we draw an i.i.d. sample\\nof $\\\\Theta(N^{\\\\delta}\\\\ln N)$ numbers uniformly at random from $1$ to $N$, where\\n$\\\\delta\\\\in (0,1)$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and\\n$0$ if it is not. Then with high probability, the MDL network fitted to this\\ndata accurately answers whether a newly drawn number between $1$ and $N$ is a\\nprime or not, with test error $\\\\leq O(N^{-\\\\delta})$. Note that the network is\\nnot designed to detect primes; minimum description learning discovers a network\\nwhich does so.', 'pdf_url': 'http://arxiv.org/pdf/2409.12446v2', 'entry_id': 'http://arxiv.org/abs/2409.12446v2', 'recommended': 1, 'referenceCount': 56, 'citationCount': 0, 'references': \"Understanding Deep Learning via Notions of Rank|Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot|How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad|Transformers Can Represent n-gram Language Models|Neural Redshift: Random Networks are not Random Functions|On Provable Length and Compositional Generalization|Learning Universal Predictors|A provably stable neural network Turing Machine with finite precision and time|What Formal Languages Can Transformers Express? A Survey|Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining|On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks|Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models|Spin glass theory and its new challenge: structured disorder|Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection|Representational Strengths and Limitations of Transformers|Do deep neural networks have an inbuilt Occam's razor?|The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning|Interpolation Learning With Minimum Description Length|Looped Transformers as Programmable Computers|Generalization on the Unseen, Logic Reasoning and Degree Curriculum|Tracr: Compiled Transformers as a Laboratory for Interpretability|Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions|Transformers Learn Shortcuts to Automata|Minimum Description Length Recurrent Neural Networks|The staircase property: How hierarchical structure can guide deep learning|Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers|Thinking Like Transformers|Deep learning: a statistical viewpoint|Understanding deep learning (still) requires rethinking generalization|Language Models are Few-Shot Learners|Transformers as Soft Reasoners over Language|Modeling the Influence of Data Structure on Learning in Neural Networks: The Hidden Manifold Model|Neural networks are a priori biased towards Boolean functions with low entropy|Nonparametric Regression on Low-Dimensional Manifolds using Deep ReLU Networks|On the Turing Completeness of Modern Neural Network Architectures|Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data|Deep learning generalizes because the parameter-function map is biased towards simple functions|A Provably Correct Algorithm for Deep Learning that Actually Works|Recurrent Neural Networks as Weighted Language Recognizers|SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data|Attention is All you Need|The Minimum Description Length Principle in Coding and Modeling|Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability|Computational power of neural networks: a characterization in terms of Kolmogorov complexity|On the computational power of neural nets|A UNIVERSAL PRIOR FOR INTEGERS AND ESTIMATION BY MINIMUM DESCRIPTION LENGTH|Attention is Turing-Complete|An Introduction to Kolmogorov Complexity and Its Applications|Vapnik-Chervonenkis dimension of neural nets|VC dimension of neural networks|Flat Minima|Keeping neural networks simple by minimising the description length of weights|A Formal Theory of Inductive Inference. Part I|die Einteilung der positiven ganzen Zahlen in vier Klassen nach der Mindestzahl der zu ihrer additiven Zusammensetzung erforderlichen Quadrate|Benign overﬁtting in linear regression|( L 4 ) For each variable x ∈ V /{ x i } , create a temporary node in the neural network x old storing the current value of x . This layer has output width ≤ 2 V . 13|\", 'citations': '', 's2FieldsOfStudy': 'Computer Science|Mathematics|Computer Science|Mathematics|', 'tldr': None}\n",
      "{'hash_id': 4272131031, 'title': 'Function Gradient Approximation with Random Shallow ReLU Networks with Control Applications', 'authors': 'Andrew Lamperski, Siddharth Salapaka', 'published': datetime.datetime(2024, 10, 7, 14, 26, 49, tzinfo=datetime.timezone.utc), 'summary': 'Neural networks are widely used to approximate unknown functions in control.\\nA common neural network architecture uses a single hidden layer (i.e. a shallow\\nnetwork), in which the input parameters are fixed in advance and only the\\noutput parameters are trained. The typical formal analysis asserts that if\\noutput parameters exist to approximate the unknown function with sufficient\\naccuracy, then desired control performance can be achieved. A long-standing\\ntheoretical gap was that no conditions existed to guarantee that, for the fixed\\ninput parameters, required accuracy could be obtained by training the output\\nparameters. Our recent work has partially closed this gap by demonstrating that\\nif input parameters are chosen randomly, then for any sufficiently smooth\\nfunction, with high-probability there are output parameters resulting in\\n$O((1/m)^{1/2})$ approximation errors, where $m$ is the number of neurons.\\nHowever, some applications, notably continuous-time value function\\napproximation, require that the network approximates the both the unknown\\nfunction and its gradient with sufficient accuracy. In this paper, we show that\\nrandomly generated input parameters and trained output parameters result in\\ngradient errors of $O((\\\\log(m)/m)^{1/2})$, and additionally, improve the\\nconstants from our prior work. We show how to apply the result to policy\\nevaluation problems.', 'pdf_url': 'http://arxiv.org/pdf/2410.05071v1', 'entry_id': 'http://arxiv.org/abs/2410.05071v1', 'recommended': 1, 'referenceCount': 13, 'citationCount': 0, 'references': 'Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control|Data-Driven Control: Theory and Applications|Deep Neural Network-Based Approximate Optimal Tracking for Unknown Nonlinear Systems|Adaptive Control and Intersections with Reinforcement Learning|Safe Exploration in Model-based Reinforcement Learning using Control Barrier Functions|High‐dimensional Statistics: A Non‐asymptotic Viewpoint, Martin J.Wainwright, Cambridge University Press, 2019, xvii 552 pages, £57.99, hardback ISBN: 978‐1‐1084‐9802‐9|Optimal Adaptive Control and Differential Games by Reinforcement Learning Principles|Robust and Adaptive Control: With Aerospace Applications|Approximate dynamic programming: solving the curses of dimensionality|Approximation theory of the MLP model in neural networks|Reachability Analysis-based Safety-Critical Control using Online Fixed-Time Reinforcement Learning|Reinforcement Learning for Optimal Feedback Control|“Approxi-mate optimal indirect regulation of an unknown agent with a lyapunov-based deep neural network,”|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Engineering|Mathematics|Computer Science|Engineering|', 'tldr': 'It is shown that randomly generated input parameters and trained output parameters result in gradient errors of $O((\\\\log(m)/m)^{1/2})$, and additionally, improve the constants from the prior work.'}\n",
      "{'hash_id': 1074271699, 'title': 'Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets', 'authors': 'Yuandong Tian', 'published': datetime.datetime(2024, 10, 2, 17, 33, 26, tzinfo=datetime.timezone.utc), 'summary': 'We prove rich algebraic structures of the solution space for 2-layer neural\\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\\nin Abelian group (e.g., modular addition). Such a rich structure enables\\nanalytical construction of global optimal solutions from partial solutions that\\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\\nweight space over different numbers of hidden nodes of the 2-layer network is\\nequipped with a semi-ring algebraic structure, and the loss function to be\\noptimized consists of monomial potentials, which are ring homomorphism,\\nallowing partial solutions to be composed into global ones by ring addition and\\nmultiplication. Our experiments show that around $95\\\\%$ of the solutions\\nobtained by gradient descent match exactly our theoretical constructions.\\nAlthough the global optimizers constructed only required a small number of\\nhidden nodes, our analysis on gradient dynamics shows that\\nover-parameterization asymptotically decouples training dynamics and is\\nbeneficial. We further show that training dynamics favors simpler solutions\\nunder weight decay, and thus high-order global optimizers such as perfect\\nmemorization are unfavorable.', 'pdf_url': 'http://arxiv.org/pdf/2410.01779v2', 'entry_id': 'http://arxiv.org/abs/2410.01779v2', 'recommended': 1, 'referenceCount': 37, 'citationCount': 0, 'references': 'Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process|Pre-trained Large Language Models Use Fourier Features to Compute Addition|Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models|DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model|Learning and Leveraging World Models in Visual Representation Learning|Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition|Chain of Thought Empowers Transformers to Solve Inherently Serial Problems|ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs|TravelPlanner: A Benchmark for Real-World Planning with Language Agents|Feature emergence via margin maximization: case studies in algebraic tasks|Counting and Algorithmic Generalization with Transformers|Large Language Models Cannot Self-Correct Reasoning Yet|The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"|Explaining grokking through circuit efficiency|The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks|Faith and Fate: Limits of Transformers on Compositionality|Emergent Representations of Program Semantics in Language Models Trained on Programs|Emergence of Maps in the Memories of Blind Navigation Agents|Progress measures for grokking via mechanistic interpretability|Grokking modular arithmetic|Transformers Learn Shortcuts to Automata|Emergent Abilities of Large Language Models|Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets|Primer: Searching for Efficient Transformers for Language Modeling|Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges|GLU Variants Improve Transformer|Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning|(Preprint)|On the Power of Over-parametrization in Neural Networks with Quadratic Activation|SOLUTIONS TO SOME FUNCTIONAL EQUATIONS AND THEIR APPLICATIONS TO CHARACTERIZATION OF PROBABILITY DISTRIBUTIONS|REPRESENTATION THEORY FOR FINITE GROUPS|CHARACTERS OF FINITE ABELIAN GROUPS|Reinforcement Learning: An Introduction|Group representations in probability and statistics|The Claude 3 Model Family: Opus, Sonnet, Haiku|When can transformers count to|can’t plan, but can|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Mathematics|Computer Science|Mathematics|', 'tldr': 'It is shown that the weight space over different numbers of hidden nodes of the 2-layer network is equipped with a semi-ring algebraic structure, and the loss function to be optimized consists of monomial potentials, allowing partial solutions to be composed into global ones by ring addition and multiplication.'}\n",
      "{'hash_id': 2502552417, 'title': 'Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading', 'authors': 'Avinash Maurya, Jie Ye, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae', 'published': datetime.datetime(2024, 10, 26, 0, 43, 59, tzinfo=datetime.timezone.utc), 'summary': \"Transformers and large language models~(LLMs) have seen rapid adoption in all\\ndomains. Their sizes have exploded to hundreds of billions of parameters and\\nkeep increasing. Under these circumstances, the training of transformers is\\nvery expensive and often hits a ``memory wall'', i.e., even when using 3D\\nparallelism (pipeline, tensor, data) and aggregating the memory of many GPUs,\\nit is still not enough to hold the necessary data structures (model parameters,\\noptimizer state, gradients, activations) in GPU memory. To compensate,\\nstate-of-the-art approaches offload the optimizer state, at least partially, to\\nthe host memory and perform hybrid CPU-GPU computations. However, the\\nmanagement of the combined host-GPU memory is often suboptimal and results in\\npoor overlapping between data movements and computations. This leads to missed\\nopportunities to simultaneously leverage the interconnect bandwidth and\\ncomputational capabilities of CPUs and GPUs. In this paper, we leverage a key\\nobservation that the interleaving of the forward, backward and update phases\\ngenerate fluctuations in the GPU memory utilization, which can be exploited to\\ndynamically move a part of the optimizer state between the host and the GPU\\nmemory at each iteration. To this end, we design and implement \\\\proj, a novel\\ntechnique to split the LLM into subgroups, whose update phase is scheduled on\\neither the CPU or the GPU based on our proposed performance model that\\naddresses the trade-off between data movement cost, acceleration on the GPUs vs\\nthe CPUs, and competition for shared resources. We integrate our approach with\\nDeepSpeed and demonstrate 2.5$\\\\times$ faster iterations over state-of-the-art\\napproaches using extensive experiments.\", 'pdf_url': 'http://arxiv.org/pdf/2410.21316v1', 'entry_id': 'http://arxiv.org/abs/2410.21316v1', 'recommended': 1, 'referenceCount': 45, 'citationCount': 0, 'references': 'Understanding the Performance and Estimating the Cost of LLM Fine-Tuning|DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models|Breaking the Memory Wall: A Study of I/O Patterns and GPU Memory Utilization for Hybrid CPU-GPU Offloaded Optimizers|Distributed Training of Large Language Models|CoTrain: Efficient Scheduling for Large-Model Training upon GPU and CPU in Parallel|GPU-Enabled Asynchronous Multi-level Checkpoint Caching and Prefetching|A Survey of Large Language Models|Canary: Fault-Tolerant FaaS for Stateful Time-Sensitive Applications|GLM-130B: An Open Bilingual Pre-trained Model|Multimodal Learning With Transformers: A Survey|OPT: Open Pre-trained Transformer Language Models|GPT-NeoX-20B: An Open-Source Autoregressive Language Model|Towards Efficient I/O Scheduling for Collaborative Multi-Level Checkpointing|M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining|ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning|ZeRO-Offload: Democratizing Billion-Scale Model Training|Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity|DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters|DeepFreeze: Towards Scalable Asynchronous Checkpointing of Deep Learning Models|Training|Capuchin: Tensor-based GPU Memory Management for Deep Learning|AutoTM: Automatic Tensor Movement in Heterogeneous Memory Systems using Integer Linear Programming|PipeDream: generalized pipeline parallelism for DNN training|ZeRO: Memory optimizations Toward Training Trillion Parameter Models|Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism|Large Batch Optimization for Deep Learning: Training BERT in 76 minutes|GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism|moDNN: Memory optimal DNN training on GPUs|Horovod: fast and easy distributed deep learning in TensorFlow|Superneurons: dynamic GPU memory management for training deep neural networks|Generating Sequences With Recurrent Neural Networks|Adaptive Subgradient Methods for Online Learning and Stochastic Optimization|Dapple:Apipelineddataparallel approachfortraininglargemodels|Gradients accumulation-pytorch|A method for stochastic optimization|HPC Wire|A 176B-Parameter Open-Access Multilingual Language|This paper is included in the Proceedings of the 2022 USENIX Annual Technical Conference. Whale: Efficient Giant Model Training over Heterogeneous GPUs|Enabling Large-Scale Scientific Discovery through Sophisticated AI System|We design and implement Deep Optimizer States , a middleware that integrates our approach into widely used LLM training run-times, namely DeepSpeed [30] and Megatron [33]|Usingdeepspeedandmegatrontotrainmegatron-turing nlg530b|MIDDLEWARE ’24, December 2–6, 2024|Reducing|HuggingFace|CPU updates; and PCIe transfers with higher precision to avoid expensive memory allocation for on-the-fly precision conversion (§ 4)|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|', 'tldr': 'A novel technique to split the LLM into subgroups whose update phase is scheduled on either the CPU or the GPU based on the proposed performance model that addresses the trade-off between data movement cost, acceleration on the GPUs vs the CPUs, and competition for shared resources is designed and implemented.'}\n",
      "{'hash_id': 3354752182, 'title': 'ProTEA: Programmable Transformer Encoder Acceleration on FPGA', 'authors': 'Ehsan Kabir, Jason D. Bakos, David Andrews, Miaoqing Huang', 'published': datetime.datetime(2024, 9, 21, 1, 44, 13, tzinfo=datetime.timezone.utc), 'summary': 'Transformer neural networks (TNN) have been widely utilized on a diverse\\nrange of applications, including natural language processing (NLP), machine\\ntranslation, and computer vision (CV). Their widespread adoption has been\\nprimarily driven by the exceptional performance of their multi-head\\nself-attention block used to extract key features from sequential data. The\\nmulti-head self-attention block is followed by feedforward neural networks,\\nwhich play a crucial role in introducing non-linearity to assist the model in\\nlearning complex patterns. Despite the popularity of TNNs, there has been\\nlimited numbers of hardware accelerators targeting these two critical blocks.\\nMost prior works have concentrated on sparse architectures that are not\\nflexible for popular TNN variants. This paper introduces \\\\textit{ProTEA}, a\\nruntime programmable accelerator tailored for the dense computations of most of\\nstate-of-the-art transformer encoders. \\\\textit{ProTEA} is designed to reduce\\nlatency by maximizing parallelism. We introduce an efficient tiling of large\\nmatrices that can distribute memory and computing resources across different\\nhardware components within the FPGA. We provide run time evaluations of\\n\\\\textit{ProTEA} on a Xilinx Alveo U55C high-performance data center accelerator\\ncard. Experimental results demonstrate that \\\\textit{ProTEA} can host a wide\\nrange of popular transformer networks and achieve near optimal performance with\\na tile size of 64 in the multi-head self-attention block and 6 in the\\nfeedforward networks block when configured with 8 parallel attention heads, 12\\nlayers, and an embedding dimension of 768 on the U55C. Comparative results are\\nprovided showing \\\\textit{ProTEA} is 2.5$\\\\times$ faster than an NVIDIA Titan XP\\nGPU. Results also show that it achieves 1.3 -- 2.8$\\\\times$ speed up compared\\nwith current state-of-the-art custom designed FPGA accelerators.', 'pdf_url': 'http://arxiv.org/pdf/2409.13975v1', 'entry_id': 'http://arxiv.org/abs/2409.13975v1', 'recommended': 1, 'referenceCount': 35, 'citationCount': 0, 'references': 'Ultra Fast Transformers on FPGAs for Particle Physics Experiments|FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs|Calabash: Accelerating Attention Using a Systolic Array Chain on FPGAs|LTrans-OPU: A Low-Latency FPGA-Based Overlay Processor for Transformer Networks|Accelerating LSTM-Based High-Rate Dynamic System Models|An Efficient FPGA-Based Accelerator for Swin Transformer|High-Frequency Systolic Array-Based Transformer Accelerator on Field Programmable Gate Arrays|Accelerating Transformer Neural Networks on FPGAs for High Energy Physics Experiments|ViA: A Novel Vision-Transformer Accelerator Based on FPGA|EFA-Trans: An Efficient and Flexible Acceleration Architecture for Transformers|Optimizing Graph Neural Networks for Jet Tagging in Particle Physics on FPGAs|A length adaptive algorithm-hardware co-design of transformer on FPGA through sparse attention and dynamic pipelining|FPGA-based design and implementation of the location attention mechanism in neural networks|Accelerating Framework of Transformer by Hardware Design and Model Compression Co-Optimization|Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture|Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices|ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks|Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning|SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning|FPGA-based Low-Batch Training Accelerator for Modern CNNs Featuring High Bandwidth Memory|Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer|FTRANS: energy-efficient acceleration of transformers using FPGA|Accelerating Large Scale GCN Inference on FPGA|Alignment-Enhanced Transformer for Constraining NMT with Pre-Specified Translations|Compressing Large-Scale Transformer-Based Models: A Case Study on BERT|A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation|StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding|RoBERTa: A Robustly Optimized BERT Pretraining Approach|[DL] A Survey of FPGA-based Neural Network Inference Accelerators|Attention is All you Need|Hardware-Aware Optimizations for Deep Learning Inference on Edge Devices|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|Language Models are Unsupervised Multitask Learners|“MicroBlaze Processor Reference Guide,”|“AMD Technical Information Portal — docs|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Engineering|Computer Science|Engineering|', 'tldr': 'Experimental results demonstrate that ProTEA can host a wide range of popular transformer networks and achieve near optimal performance, and an efficient tiling of large matrices that can distribute memory and computing resources across different hardware components within the FPGA is introduced.'}\n",
      "{'hash_id': 1216912402, 'title': 'Small Language Models: Survey, Measurements, and Insights', 'authors': 'Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu', 'published': datetime.datetime(2024, 9, 24, 6, 36, 56, tzinfo=datetime.timezone.utc), 'summary': 'Small language models (SLMs), despite their widespread adoption in modern\\nsmart devices, have received significantly less academic attention compared to\\ntheir large language model (LLM) counterparts, which are predominantly deployed\\nin data centers and cloud environments. While researchers continue to improve\\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\\nresearch aims to make machine intelligence more accessible, affordable, and\\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\\nlanguage models with 100M-5B parameters, we survey 59 state-of-the-art\\nopen-source SLMs, analyzing their technical innovations across three axes:\\narchitectures, training datasets, and training algorithms. In addition, we\\nevaluate their capabilities in various domains, including commonsense\\nreasoning, in-context learning, mathematics, and coding. To gain further\\ninsight into their on-device runtime costs, we benchmark their inference\\nlatency and memory footprints. Through in-depth analysis of our benchmarking\\ndata, we offer valuable insights to advance research in this field.', 'pdf_url': 'http://arxiv.org/pdf/2409.15790v1', 'entry_id': 'http://arxiv.org/abs/2409.15790v1', 'recommended': 1, 'referenceCount': 51, 'citationCount': 1, 'references': 'What is the Role of Small Models in the LLM Era: A Survey|On-Device Language Models: A Comprehensive Review|Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU|The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale|PowerInfer-2: Fast Large Language Model Inference on a Smartphone|Large Language Models on Mobile Devices: Measurements, Analysis, and Insights|Achieving Sparse Activation in Small Language Models|Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning|A Careful Examination of Large Language Model Performance on Grade School Arithmetic|Small Language Models Are Good Too: An Empirical Study of Zero-Shot Classification|Scaling Laws for Fine-Grained Mixture of Experts|Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research|A Survey of Resource-efficient LLM and Multimodal Foundation Models|LLM in a flash: Efficient Large Language Model Inference with Limited Memory|Mamba: Linear-Time Sequence Modeling with Selective State Spaces|CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages|Mobile Foundation Model as Firmware|EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models|FwdLLM: Efficient FedLLM using Forward Gradient|Mini-Giants: \"Small\" Language Models and Open Source Win-Win|The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only|RWKV: Reinventing RNNs for the Transformer Era|Solving Quantitative Reasoning Problems with Language Models|Training Compute-Optimal Large Language Models|Efficient Large Scale Language Modeling with Mixtures of Experts|Training Verifiers to Solve Math Word Problems|TruthfulQA: Measuring How Models Mimic Human Falsehoods|The Pile: An 800GB Dataset of Diverse Text for Language Modeling|It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners|Measuring Massive Multitask Language Understanding|Scaling Laws for Neural Language Models|The Pushshift Reddit Dataset|PIQA: Reasoning about Physical Commonsense in Natural Language|Microsoft|BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions|HellaSwag: Can a Machine Really Finish Your Sentence?|Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering|Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge|Facebook|May the source be with you.|Databricks|WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models|Winogrande: An adversarial wino-grad schema challenge at scale|CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge|Google ai edge sdk for gemini nano|BigScience|TensorOpera|HuggingFace|Introducing apple’s on-device and server foundation models|AllenAI|Together Computer|', 'citations': 'MiniPLM: Knowledge Distillation for Pre-Training Language Models|', 's2FieldsOfStudy': 'Computer Science|Computer Science|Linguistics|', 'tldr': 'This work surveys 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms, and evaluates their capabilities in various domains, including commonsense reasoning, in-context learning, mathematics, and coding.'}\n",
      "{'hash_id': 2324851006, 'title': 'Search for Efficient Large Language Models', 'authors': 'Xuan Shen, Pu Zhao, Yifan Gong, Zhenglun Kong, Zheng Zhan, Yushu Wu, Ming Lin, Chao Wu, Xue Lin, Yanzhi Wang', 'published': datetime.datetime(2024, 9, 25, 21, 32, 12, tzinfo=datetime.timezone.utc), 'summary': 'Large Language Models (LLMs) have long held sway in the realms of artificial\\nintelligence research. Numerous efficient techniques, including weight pruning,\\nquantization, and distillation, have been embraced to compress LLMs, targeting\\nmemory reduction and inference acceleration, which underscore the redundancy in\\nLLMs. However, most model compression techniques concentrate on weight\\noptimization, overlooking the exploration of optimal architectures. Besides,\\ntraditional architecture search methods, limited by the elevated complexity\\nwith extensive parameters, struggle to demonstrate their effectiveness on LLMs.\\nIn this paper, we propose a training-free architecture search framework to\\nidentify optimal subnets that preserve the fundamental strengths of the\\noriginal LLMs while achieving inference acceleration. Furthermore, after\\ngenerating subnets that inherit specific weights from the original LLMs, we\\nintroduce a reformation algorithm that utilizes the omitted weights to rectify\\nthe inherited weights with a small amount of calibration data. Compared with\\nSOTA training-free structured pruning works that can generate smaller networks,\\nour method demonstrates superior performance across standard benchmarks.\\nFurthermore, our generated subnets can directly reduce the usage of GPU memory\\nand achieve inference acceleration. Code:\\nhttps://github.com/shawnricecake/search-llm', 'pdf_url': 'http://arxiv.org/pdf/2409.17372v2', 'entry_id': 'http://arxiv.org/abs/2409.17372v2', 'recommended': 1, 'referenceCount': 54, 'citationCount': 0, 'references': 'Pruning Foundation Models for High Accuracy without Retraining|Exploring Token Pruning in Vision State Space Models|EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge|SliceGPT: Compress Large Language Models by Deleting Rows and Columns|Fluctuation-based Adaptive Structured Pruning for Large Language Models|Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge|Fast and Fair Medical AI on the Edge Through Neural Architecture Search for Hybrid Vision Models|Late Breaking Results: Fast Fair Medical Applications? Hybrid Vision Models Achieve the Fairness on the Edge|Towards Real-Time Segmentation on the Edge|A Simple and Effective Pruning Approach for Large Language Models|Judging LLM-as-a-judge with MT-Bench and Chatbot Arena|PerfHD: Efficient ViT Architecture Performance Ranking using Hyperdimensional Computing|Pruning Parameterization with Bi-level Optimization for Efficient Semantic Segmentation on the Edge|LLM-Pruner: On the Structural Pruning of Large Language Models|DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network|LLaMA: Open and Efficient Foundation Language Models|SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot|SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models|BLOOM: A 176B-Parameter Open-Access Multilingual Language Model|Advancing Model Pruning via Bi-level Optimization|Compiler-Aware Neural Architecture Search for On-Mobile Real-time Super-Resolution|Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization|OPT: Open Pre-trained Transformer Language Models|Searching for Efficient Multi-Stage Vision Transformers|Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search|Vision Transformer Architecture Search|Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains|An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale|Contrastive Distillation on Intermediate Representations for Language Model Compression|Language Models are Few-Shot Learners|A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration Framework|PIQA: Reasoning about Physical Commonsense in Natural Language|Patient Knowledge Distillation for BERT Model Compression|BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions|HellaSwag: Can a Machine Really Finish Your Sentence?|Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering|An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks|Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge|Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning|Pointer Sentinel Mixture Models|Fast Alternating Direction Optimization Methods|Proximal Algorithms|Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers|Building a Large Annotated Corpus of English: The Penn Treebank|HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception|NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training|Autoformer: Searching trans-formers for visual recognition|Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition|Woodﬁsher: Efﬁcient second-order approximation for neural network compression|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|An Adversarial Winograd Schema Challenge at Scale|Language Models are Unsupervised Multitask Learners|Efﬁcientnet: Rethinking model scaling for convolutional neural networks|GPTQ: Accurate post-training compression for generative pretrained transformers|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|', 'tldr': 'A training-free architecture search framework to identify optimal subnets that preserve the fundamental strengths of the original LLMs while achieving inference acceleration, and introduces a reformation algorithm that utilizes the omitted weights to rectify the inherited weights with a small amount of calibration data.'}\n",
      "{'hash_id': 3648476895, 'title': 'Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge', 'authors': 'Jiahuan Li, Yiqing Cao, Shujian Huang, Jiajun Chen', 'published': datetime.datetime(2024, 10, 7, 6, 49, 41, tzinfo=datetime.timezone.utc), 'summary': \"Having been trained on massive pretraining data, large language models have\\nshown excellent performance on many knowledge-intensive tasks. However,\\npretraining data tends to contain misleading and even conflicting information,\\nand it is intriguing to understand how LLMs handle these noisy data during\\ntraining. In this study, we systematically analyze LLMs' learning preferences\\nfor data with conflicting knowledge. We find that pretrained LLMs establish\\nlearning preferences similar to humans, i.e., preferences towards formal texts\\nand texts with fewer spelling errors, resulting in faster learning and more\\nfavorable treatment of knowledge in data with such features when facing\\nconflicts. This finding is generalizable across models and languages and is\\nmore evident in larger models. An in-depth analysis reveals that LLMs tend to\\ntrust data with features that signify consistency with the majority of data,\\nand it is possible to instill new preferences and erase old ones by\\nmanipulating the degree of consistency with the majority data.\", 'pdf_url': 'http://arxiv.org/pdf/2410.04784v1', 'entry_id': 'http://arxiv.org/abs/2410.04784v1', 'recommended': 1, 'referenceCount': 17, 'citationCount': 0, 'references': 'Llama 2: Open Foundation and Fine-Tuned Chat Models|KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment|Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond|Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling|ChatGPT outperforms crowd workers for text-annotation tasks|GPT-4 Technical Report|Large language models encode clinical knowledge|Locating and Editing Factual Associations in GPT|Transformer Feed-Forward Layers Are Key-Value Memories|What Does BERT Learn about the Structure of Language?|ChatGPT Goes to Law School|2024. A comprehensive study of knowledge editing for large language models|2023c. Emotional intelligence of large language models|2023. Head-to-tail: How knowl-edgeable are large language models (llm)? aka will llms replace knowledge graphs?|Open large-scale language models|2022. Knowledge neurons in pretrained transformers|style, scientific reports style and wikipedia style are more preferred by the model. 5320|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|Linguistics|', 'tldr': 'It is found that pretrained LLMs establish learning preferences similar to humans, i.e., preferences towards formal texts and texts with fewer spelling errors, resulting in faster learning and more favorable treatment of knowledge in data with such features when facing conflicts.'}\n",
      "{'hash_id': 3477821811, 'title': 'Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives', 'authors': 'Hao Sun, Yunyi Shen, Jean-Francois Ton', 'published': datetime.datetime(2024, 11, 7, 18, 57, 3, tzinfo=datetime.timezone.utc), 'summary': 'The Bradley-Terry (BT) model is a common and successful practice in reward\\nmodeling for Large Language Model (LLM) alignment. However, it remains unclear\\nwhy this model -- originally developed for multi-player stochastic game\\nmatching -- can be adopted to convert pairwise response comparisons to reward\\nvalues and make predictions. Especially given the fact that only a limited\\nnumber of prompt-response pairs are sparsely compared with others. In this\\npaper, we first revisit the foundations of using BT models in reward modeling,\\nand establish the convergence rate of BT reward models based on deep neural\\nnetworks using embeddings, providing a theoretical foundation for their use.\\nDespite theoretically sound, we argue that the BT model is not a necessary\\nchoice from the perspective of downstream optimization. This is because a\\nreward model only needs to preserve the correct ranking predictions through a\\nmonotonic transformation of the true reward. We highlight the critical concept\\nof order consistency in reward modeling and demonstrate that the BT model\\npossesses this property. Consequently, we propose a simple and straightforward\\nupper-bound algorithm, compatible with off-the-shelf binary classifiers, as an\\nalternative order-consistent reward modeling objective. To offer practical\\ninsights, we empirically evaluate the performance of these different reward\\nmodeling approaches across more than 12,000 experimental setups, using $6$ base\\nLLMs, $2$ datasets, and diverse annotation designs that vary in quantity,\\nquality, and pairing choices in preference annotations.', 'pdf_url': 'http://arxiv.org/pdf/2411.04991v1', 'entry_id': 'http://arxiv.org/abs/2411.04991v1', 'recommended': 1, 'referenceCount': 0, 'citationCount': 0, 'references': '', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|', 'tldr': 'This paper revisits the foundations of using BT models in reward modeling, establishes the convergence rate of BT reward models based on deep neural networks using embeddings, and proposes a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective.'}\n",
      "{'hash_id': 2652717167, 'title': \"What's New in My Data? Novelty Exploration via Contrastive Generation\", 'authors': 'Masaru Isonuma, Ivan Titov', 'published': datetime.datetime(2024, 10, 18, 15, 24, 5, tzinfo=datetime.timezone.utc), 'summary': 'Fine-tuning is widely used to adapt language models for specific goals, often\\nleveraging real-world data such as patient records, customer-service\\ninteractions, or web content in languages not covered in pre-training. These\\ndatasets are typically massive, noisy, and often confidential, making their\\ndirect inspection challenging. However, understanding them is essential for\\nguiding model deployment and informing decisions about data cleaning or\\nsuppressing any harmful behaviors learned during fine-tuning. In this study, we\\nintroduce the task of novelty discovery through generation, which aims to\\nidentify novel properties of a fine-tuning dataset by generating examples that\\nillustrate these properties. Our approach, Contrastive Generative Exploration\\n(CGE), assumes no direct access to the data but instead relies on a pre-trained\\nmodel and the same model after fine-tuning. By contrasting the predictions of\\nthese two models, CGE can generate examples that highlight novel\\ncharacteristics of the fine-tuning data. However, this simple approach may\\nproduce examples that are too similar to one another, failing to capture the\\nfull range of novel phenomena present in the dataset. We address this by\\nintroducing an iterative version of CGE, where the previously generated\\nexamples are used to update the pre-trained model, and this updated model is\\nthen contrasted with the fully fine-tuned model to generate the next example,\\npromoting diversity in the generated outputs. Our experiments demonstrate the\\neffectiveness of CGE in detecting novel content, such as toxic language, as\\nwell as new natural and programming languages. Furthermore, we show that CGE\\nremains effective even when models are fine-tuned using differential privacy\\ntechniques.', 'pdf_url': 'http://arxiv.org/pdf/2410.14765v1', 'entry_id': 'http://arxiv.org/abs/2410.14765v1', 'recommended': 1, 'referenceCount': 52, 'citationCount': 0, 'references': \"The Llama 3 Herd of Models|Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities|DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation|Latxa: An Open Language Model and Evaluation Suite for Basque|ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding|SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding|Navigating the OverKill in Large Language Models|Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback|Tuning Language Models by Proxy|The Falcon Series of Open Language Models|Oasis: Data Curation and Assessment System for Pretraining of Large Language Models|What's In My Big Data?|Detecting Pretraining Data from Large Language Models|Contrastive Decoding Improves Reasoning in Large Language Models|Time Travel in LLMs: Tracing Data Contamination in Large Language Models|Large language models in medicine|GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration|The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only|Data Portraits: Recording Foundation Model Training Data|The ROOTS Search Tool: Data Transparency for LLMs|LLaMA: Open and Efficient Foundation Language Models|Don’t Look at the Data! How Differential Privacy Reconfigures the Practices of Data Science|Dataset Distillation: A Comprehensive Review|Data Distillation: A Survey|Lessons Learned: Surveying the Practicality of Differential Privacy in the Industry|Contrastive Decoding: Open-ended Text Generation as Optimization|Knowledge Unlearning for Mitigating Privacy Risks in Language Models|ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection|A large language model for electronic health records|Generalized Out-of-Distribution Detection: A Survey|Differentially Private Fine-tuning of Language Models|Large Language Models Can Be Strong Differentially Private Learners|On the Importance of Gradients for Detecting Distributional Shifts in the Wild|LoRA: Low-Rank Adaptation of Large Language Models|DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts|Data Distillation for Text Classification|Dataset Condensation with Differentiable Siamese Augmentation|Energy-based Out-of-distribution Detection|Dataset Condensation with Gradient Matching|Generalized ODIN: Detecting Out-of-Distribution Image Without Learning From Out-of-Distribution Data|Soft-Label Dataset Distillation and Text Dataset Distillation|A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks|Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks|Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles|A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks|Deep Learning with Differential Privacy|Adam: A Method for Stochastic Optimization|Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds|Stochastic gradient descent with differentially private updates|Dataset Distillation with Attention Labels for Fine-tuning BERT|: An open reproduction of llama|realistic use case where novel domains are not entirely new but significantly underrepresented|\", 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|', 'tldr': 'This study introduces the task of novelty discovery through generation, which aims to identify novel properties of a fine-tuning dataset by generating examples that illustrate these properties, and introduces an iterative version of CGE, which relies on a pre-trained model and the same model after fine-tuning.'}\n",
      "{'hash_id': 1497242042, 'title': 'Fantastic LLMs for Preference Data Annotation and How to (not) Find Them', 'authors': 'Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava', 'published': datetime.datetime(2024, 11, 4, 18, 54, 39, tzinfo=datetime.timezone.utc), 'summary': 'Preference tuning of large language models (LLMs) relies on high-quality\\nhuman preference data, which is often expensive and time-consuming to gather.\\nWhile existing methods can use trained reward models or proprietary model as\\njudges for preference annotation, they have notable drawbacks: training reward\\nmodels remain dependent on initial human data, and using proprietary model\\nimposes license restrictions that inhibits commercial usage. In this paper, we\\nintroduce customized density ratio (CDR) that leverages open-source LLMs for\\ndata annotation, offering an accessible and effective solution. Our approach\\nuses the log-density ratio between a well-aligned LLM and a less aligned LLM as\\na reward signal. We explores 221 different LLMs pairs and empirically\\ndemonstrate that increasing the performance gap between paired LLMs correlates\\nwith better reward generalization. Furthermore, we show that tailoring the\\ndensity ratio reward function with specific criteria and preference exemplars\\nenhances performance across domains and within target areas.\\n  In our experiment using density ratio from a pair of Mistral-7B models, CDR\\nachieves a RewardBench score of 82.6, outperforming the best in-class trained\\nreward functions and demonstrating competitive performance against SoTA models\\nin Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an\\non-policy preference dataset with which we preference tune Llama-3-8B-Instruct\\nwith SimPO. The final model achieves a 37.4% (+15.1%) win rate on ArenaHard and\\na 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a\\nscore of 8.0 on MT-Bench.', 'pdf_url': 'http://arxiv.org/pdf/2411.02481v1', 'entry_id': 'http://arxiv.org/abs/2411.02481v1', 'recommended': 1, 'referenceCount': 45, 'citationCount': 0, 'references': 'TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights|Direct Judgement Preference Optimization|On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization|Generative Verifiers: Reward Modeling as Next-Token Prediction|Self-Taught Evaluators|Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning|From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline|Nemotron-4 340B Technical Report|Bootstrapping Language Models with DPO Implicit Rewards|HelpSteer2: Open-source dataset for training top-performing reward models|Distributional Preference Alignment of LLMs via Optimal Transport|SimPO: Simple Preference Optimization with a Reference-Free Reward|RLHF Workflow: From Reward Modeling to Online RLHF|Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models|Self-Play Preference Optimization for Language Model Alignment|Iterative Reasoning Preference Optimization|DPO Meets PPO: Reinforced Token Optimization for RLHF|Weak-to-Strong Extrapolation Expedites Alignment|Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators|Disentangling Length from Quality in Direct Preference Optimization|RewardBench: Evaluating Reward Models for Language Modeling|ORPO: Monolithic Preference Optimization without Reference Model|KTO: Model Alignment as Prospect Theoretic Optimization|Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation|A Minimaximalist Approach to Reinforcement Learning from Human Feedback|Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss|Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint|Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision|A General Theoretical Paradigm to Understand Learning from Human Preferences|ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback|Contrastive Decoding Improves Reasoning in Large Language Models|DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models|Llama 2: Open Foundation and Fine-Tuned Chat Models|Direct Preference Optimization: Your Language Model is Secretly a Reward Model|SLiC-HF: Sequence Likelihood Calibration with Human Feedback|RRHF: Rank Responses to Align Language Models with Human Feedback without tears|Contrastive Decoding: Open-ended Text Generation as Optimization|Training language models to follow instructions with human feedback|Proximal Policy Optimization Algorithms|WHO Technical Report|RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS|A Grounded Preference Model for LLM Alignment|E I LLUSTRATION OF C USTOMIZED D ENSITY R|NousResearch|Skywork reward model series|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Computer Science|', 'tldr': 'Customized density ratio (CDR) is introduced that leverages open-source LLMs for data annotation, offering an accessible and effective solution and it is shown that tailoring the density ratio reward function with specific criteria and preference exemplars enhances performance across domains and within target areas.'}\n",
      "{'hash_id': 1060650794, 'title': 'Learning Chaotic Dynamics with Embedded Dissipativity', 'authors': 'Sunbochen Tang, Themistoklis Sapsis, Navid Azizan', 'published': datetime.datetime(2024, 10, 1, 18, 3, 14, tzinfo=datetime.timezone.utc), 'summary': 'Chaotic dynamics, commonly seen in weather systems and fluid turbulence, are\\ncharacterized by their sensitivity to initial conditions, which makes accurate\\nprediction challenging. Despite its sensitivity to initial perturbations, many\\nchaotic systems observe dissipative behaviors and ergodicity. Therefore,\\nrecently various approaches have been proposed to develop data-driven models\\npreserving invariant statistics over long horizons. Although these methods have\\nshown empirical success in reducing instances of unbounded trajectory\\ngeneration, many of the models are still prone to generating unbounded\\ntrajectories, leading to invalid statistics evaluation. In this paper, we\\npropose a novel neural network architecture that simultaneously learns a\\ndissipative dynamics emulator that guarantees to generate bounded trajectories\\nand an energy-like function that governs the dissipative behavior. More\\nspecifically, by leveraging control-theoretic ideas, we derive algebraic\\nconditions based on the learned energy-like function that ensure asymptotic\\nconvergence to an invariant level set. Using these algebraic conditions, our\\nproposed model enforces dissipativity through a ReLU projection layer, which\\nprovides formal trajectory boundedness guarantees. Furthermore, the invariant\\nlevel set provides an outer estimate for the strange attractor, which is known\\nto be very difficult to characterize due to its complex geometry. We\\ndemonstrate the capability of our model in producing bounded long-horizon\\ntrajectory forecasts and characterizing the attractor for chaotic dynamical\\nsystems including Lorenz 96 and a truncated Kuramoto-Sivashinsky equation.', 'pdf_url': 'http://arxiv.org/pdf/2410.00976v2', 'entry_id': 'http://arxiv.org/abs/2410.00976v2', 'recommended': 1, 'referenceCount': 26, 'citationCount': 0, 'references': 'Constraining chaos: Enforcing dynamical invariants in the training of reservoir computers.|Training neural operators to preserve invariant measures of chaotic attractors|Data-Driven Control with Inherent Lyapunov Stability|Stabilizing Machine Learning Prediction of Dynamics: Noise and Noise-inspired Regularization|On the difficulty of learning chaotic dynamics with RNNs|Reducing network size and improving prediction stability of reservoir computing.|Stability Analysis of Reservoir Computers Dynamics via Lyapunov Functions|Attractor reconstruction by machine learning.|Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.|Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering|Turbulence, Coherent Structures, Dynamical Systems and Symmetry|Semidefinite Optimization and Convex Algebraic Geometry|FUNDAMENTAL LIMITATIONS OF AD HOC LINEAR AND QUADRATIC MULTI-LEVEL REGRESSION MODELS FOR PHYSICAL SYSTEMS|Chaos: From Simple Models To Complex Systems|COMPUTATIONAL STUDY OF CHAOTIC AND ORDERED SOLUTIONS OF THE KURAMOTO-SIVASHINSKY EQUATION|Approximation by superpositions of a sigmoidal function|On the concept of attractor|Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields|Diffusion-Induced Chaos in Reaction Systems|Deterministic nonperiodic flow|Learning Chaotic Dynamics in Dissipative Systems|Chaos In Dynamical Systems|Physics constrained nonlinear regression models for time series|of nonlinear systems|Dynamical systems and numerical analysis , volume 2|Nonlinear analysis of hydrodynamic instability in laminar flames—I. Derivation of basic equations|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Engineering|Physics|Computer Science|', 'tldr': 'A novel neural network architecture is proposed that simultaneously learns a dissipative dynamics emulator that guarantees to generate bounded trajectories and an energy-like function that governs the dissipative behavior and derives algebraic conditions based on the learned energy-like function that ensure asymptotic convergence to an invariant level set.'}\n",
      "{'hash_id': 1820603686, 'title': 'ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence', 'authors': 'Wenjie Mei, Dongzhe Zheng, Shihua Li', 'published': datetime.datetime(2024, 11, 4, 17, 20, 42, tzinfo=datetime.timezone.utc), 'summary': 'Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can\\nprocess data without the limitation of time intervals. They have advantages in\\nlearning and understanding the evolution of complex real dynamics. Many\\nprevious works have focused on NODEs in concise forms, while numerous physical\\nsystems taking straightforward forms, in fact, belong to their more complex\\nquasi-classes, thus appealing to a class of general NODEs with high scalability\\nand flexibility to model those systems. This, however, may result in intricate\\nnonlinear properties. In this paper, we introduce ControlSynth Neural ODEs\\n(CSODEs). We show that despite their highly nonlinear nature, convergence can\\nbe guaranteed via tractable linear inequalities. In the composition of CSODEs,\\nwe introduce an extra control term for learning the potential simultaneous\\ncapture of dynamics at different scales, which could be particularly useful for\\npartial differential equation-formulated systems. Finally, we compare several\\nrepresentative NNs with CSODEs on important physical dynamics under the\\ninductive biases of CSODEs, and illustrate that CSODEs have better learning and\\npredictive abilities in these settings.', 'pdf_url': 'http://arxiv.org/pdf/2411.02292v1', 'entry_id': 'http://arxiv.org/abs/2411.02292v1', 'recommended': 1, 'referenceCount': 27, 'citationCount': 0, 'references': 'Towards complex dynamic physics system simulation with graph neural ordinary equations|Stabilized Neural Differential Equations for Learning Dynamics with Explicit Constraints|A Neural ODE Interpretation of Transformer Layers|On annular short-time stability conditions for generalized Persidskii systems|Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks|Stochastic physics-informed neural ordinary differential equations|Physics-informed neural networks for the shallow-water equations on the sphere|Deep Learning Adapted to Differential Neural Networks Used as Pattern Classification of Electrophysiological Signals|On Second Order Behaviour in Augmented Neural ODEs|Neural Controlled Differential Equations for Irregular Time Series|Stable Neural Flows|Lagrangian Neural Networks|Dissecting Neural ODEs|Graph Neural Ordinary Differential Equations|Hamiltonian Neural Networks|Augmented Neural ODEs|Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations|Neural Ordinary Differential Equations|Functional multi-layer perceptron: a non-linear tool for functional data analysis|On characterizations of the input-to-state stability property|Finding Structure in Time|Learning representations by back-propagating errors|A model of neuronal bursting using three coupled first order differential equations|Parametric Correspondence and Chamfer Matching: Two New Techniques for Image Matching|Water Waves: The Mathematical Theory with Applications|Computer simulation of liquids|Autocatalytic reactions in the isothermal, continuous stirred tank reactor: Isolas and other forms of multistability|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Engineering|Computer Science|Physics|Engineering|', 'tldr': 'It is shown that despite their highly nonlinear nature, convergence can be guaranteed via tractable linear inequalities in CSODEs, and it is illustrated that CSODEs have better learning and predictive abilities in these settings.'}\n",
      "{'hash_id': 2914289560, 'title': 'Performance Analysis of 6TiSCH Networks Using Discrete Events Simulator', 'authors': 'Guilherme de Santi Peron, Marcos Eduardo Pivaro Monteiro, João Luís Verdegay de Barros, Jamil Farhat, Glauber Brante', 'published': datetime.datetime(2024, 10, 4, 12, 51, 42, tzinfo=datetime.timezone.utc), 'summary': 'The Internet of Things (IoT) empowers small devices to sense, react, and\\ncommunicate, with applications ranging from smart ordinary household objects to\\ncomplex industrial processes. To provide access to an increasing number of IoT\\ndevices, particularly in long-distance communication scenarios, a robust\\nlow-power wide area network (LPWAN) protocol becomes essential. A widely\\nadopted protocol for this purpose is 6TiSCH, which builds upon the IEEE\\n802.15.4 standard. It introduces time-slotted channel hopping (TSCH) mode as a\\nnew medium access control (MAC) layer operating mode, in conjunction with IEEE\\n802.15.4g, which also defines both MAC and physical layer (PHY) layers and\\nprovides IPv6 connectivity for LPWAN. Notably, 6TiSCH has gained adoption in\\nsignificant standards such as Wireless Intelligent Ubiquitous Networks\\n(Wi-SUN). This study evaluates the scalability of 6TiSCH, with a focus on key\\nparameters such as queue size, the maximum number of single-hop retries, and\\nthe slotframe length. Computational simulations were performed using an\\nopen-source simulator and obtained the following results: increasing the\\ntransmission queue size, along with adjusting the number of retries and\\nslotframe length, leads to a reduction in the packet error rate (PER). Notably,\\nthe impact of the number of retries is particularly pronounced. Furthermore,\\nthe effect on latency varies based on the specific combination of these\\nparameters as the network scales.', 'pdf_url': 'http://arxiv.org/pdf/2410.03383v2', 'entry_id': 'http://arxiv.org/abs/2410.03383v2', 'recommended': 1, 'referenceCount': 17, 'citationCount': 0, 'references': 'Co-simulation platform for the assessment of transactive energy systems|A Survey of IoT and Machine Learning Based Monitoring of the Growth of Crops Using Blockchain Technology|LoRaWAN vs. 6TiSCH: Which one scales better?|TSCH-Sim: Scaling Up Simulations of TSCH and 6TiSCH Networks|A Comprehensive Survey on Internet of Things (IoT) Toward 5G Wireless Systems|Simulating 6TiSCH networks|Minimal IPv6 over the TSCH Mode of IEEE 802.15.4e (6TiSCH) Configuration|Lessons learned from large-scale dense IEEE802.15.4 connectivity traces|Decentralized Traffic Aware Scheduling in 6TiSCH Networks: Design and Experimental Evaluation|Wireless Communications|A Survey of Public IoT Datasets for Network Security Research|A Survey on IoT-Enabled Home Automation Systems: Attacks and Defenses|Balancing Decentralization for Restoration in Power Distribution Systems With Agents|“CC1352R simplelink high-performance multi-band wireless MCU,”|An industrial IoT MAC protocol based on IEEE 802.15.4e TSCH for a large-scale network|On-the-Fly Bandwidth Reservation for 6TiSCH Wireless Industrial Networks|“IEEE standard for local and metropolitan area networks–part 15.4: Low-rate wireless personal area networks (lr-wpans) amendment|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Engineering|Computer Science|Engineering|', 'tldr': 'This study evaluates the scalability of 6TiSCH with a focus on key parameters such as queue size, the maximum number of single-hop retries, and the slotframe length, finding that increasing the transmission queue size, along with adjusting the number of retries and slotframe length, leads to a reduction in the packet error rate (PER).'}\n",
      "{'hash_id': 3783855683, 'title': 'Prevailing against Adversarial Noncentral Disturbances: Exact Recovery of Linear Systems with the $l_1$-norm Estimator', 'authors': 'Jihun Kim, Javad Lavaei', 'published': datetime.datetime(2024, 10, 4, 8, 7, 31, tzinfo=datetime.timezone.utc), 'summary': 'This paper studies the linear system identification problem in the general\\ncase where the disturbance is sub-Gaussian, correlated, and possibly\\nadversarial. First, we consider the case with noncentral (nonzero-mean)\\ndisturbances for which the ordinary least-squares (OLS) method fails to\\ncorrectly identify the system. We prove that the $l_1$-norm estimator\\naccurately identifies the system under the condition that each disturbance has\\nequal probabilities of being positive or negative. This condition restricts the\\nsign of each disturbance but allows its magnitude to be arbitrary. Second, we\\nconsider the case where each disturbance is adversarial with the model that the\\nattack times happen occasionally but the distributions of the attack values are\\ncompletely arbitrary. We show that when the probability of having an attack at\\na given time is less than 0.5, the $l_1$-norm estimator prevails against any\\nadversarial noncentral disturbances and the exact recovery is achieved within a\\nfinite time. These results pave the way to effectively defend against\\narbitrarily large noncentral attacks in safety-critical systems.', 'pdf_url': 'http://arxiv.org/pdf/2410.03218v2', 'entry_id': 'http://arxiv.org/abs/2410.03218v2', 'recommended': 1, 'referenceCount': 25, 'citationCount': 0, 'references': 'Exact Recovery Guarantees for Parameterized Non-linear System Identification Problem under Adversarial Attacks|Exact Recovery for System Identification with More Corrupt Data than Clean Data|Strengthening Transmission System Resilience Against Extreme Weather Events by Undergrounding Selected Lines|A Survey of Cyber Attacks on Cyber Physical Systems: Recent Advances and Challenges|Learning of Dynamical Systems under Adversarial Attacks|On the role of present bias and biased price beliefs in household energy consumption|High-Dimensional Probability: An Introduction with Applications in Data Science|Black-Box Control for Linear Dynamical Systems|Data-Driven Transmission Defense Planning Against Extreme Weather Events|Finite-time Identification of Stable Linear Systems Optimality of the Least-Squares Estimator|Naive Exploration is Optimal for Online LQR|Improper Learning for Non-Stochastic Control|The Nonstochastic Control Problem|Certainty Equivalence is Efficient for Linear Quadratic Control|Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification|Research on Resilience of Power Systems Under Natural Disasters—A Review|Attack Detection and Identification in Cyber-Physical Systems|Catastrophic cascade of failures in interdependent networks|The Adaptive Markets Hypothesis|Robust Adaptive Control|Least Squares Estimates in Stochastic Regression Models with Applications to Identification and Control of Dynamic Systems|On strong consistency of least squares identification algorithms|Consistency of the least-squares identification method|Finite Time LTI System Identification|Stochastic Systems: Estimation, Identification, and Adaptive Control|', 'citations': '', 's2FieldsOfStudy': 'Mathematics|Mathematics|Engineering|', 'tldr': None}\n",
      "{'hash_id': 3526584350, 'title': 'Safety of Linear Systems under Severe Sensor Attacks', 'authors': 'Xiao Tan, Pio Ong, Paulo Tabuada, Aaron D. Ames', 'published': datetime.datetime(2024, 9, 12, 22, 0, 54, tzinfo=datetime.timezone.utc), 'summary': 'Cyber-physical systems can be subject to sensor attacks, e.g., sensor\\nspoofing, leading to unsafe behaviors. This paper addresses this problem in the\\ncontext of linear systems when an omniscient attacker can spoof several system\\nsensors at will. In this adversarial environment, existing results have derived\\nnecessary and sufficient conditions under which the state estimation problem\\nhas a unique solution. In this work, we consider a severe attacking scenario\\nwhen such conditions do not hold. To deal with potential state estimation\\nuncertainty, we derive an exact characterization of the set of all possible\\nstate estimates. Using the framework of control barrier functions, we propose\\ndesign principles for system safety in offline and online phases. For the\\noffline phase, we derive conditions on safe sets for all possible sensor\\nattacks that may be encountered during system deployment. For the online phase,\\nwith past system measurements collected, a quadratic program-based safety\\nfilter is proposed to enforce system safety. A 2D-vehicle example is used to\\nillustrate the theoretical results.', 'pdf_url': 'http://arxiv.org/pdf/2409.08413v1', 'entry_id': 'http://arxiv.org/abs/2409.08413v1', 'recommended': 1, 'referenceCount': 24, 'citationCount': 0, 'references': 'Belief Control Barrier Functions for Risk-Aware Control|Robust Safety under Stochastic Uncertainty with Discrete-Time Control Barrier Functions|Safe and Robust Observer-Controller Synthesis Using Control Barrier Functions|Safe Control for Nonlinear Systems under Faults and Attacks via Control Barrier Functions|Safe Control for Nonlinear Systems With Stochastic Uncertainty via Risk Control Barrier Functions|On the Computational Complexity of the Secure State-Reconstruction Problem|Guaranteeing Safety of Learned Perception Modules via Measurement-Robust Control Barrier Functions|A Coding Theoretic View of Secure State Reconstruction|Injected and Delivered: Fabricating Implicit Control over Actuation Systems by Spoofing Inertial Sensors|Input-to-State Safety With Control Barrier Functions|Discrete Control Barrier Functions for Safety-Critical Control of Discrete Systems with Application to Bipedal Robot Navigation|Control Barrier Function Based Quadratic Programs for Safety Critical Systems|Observability of linear systems under adversarial attacks|Input-to-State Stabilizing Control Under Denial-of-Service|Covert Misappropriation of Networked Control Systems: Presenting a Feedback Structure|Event-Triggered State Observers for Sparse Sensor Noise/Attacks|Non-invasive Spoofing Attacks for Anti-lock Braking Systems|On the Performance Analysis of Resilient Networked Control Systems Under Replay Attacks|Secure Estimation and Control for Cyber-Physical Systems Under Adversarial Attacks|False data injection attacks against state estimation in wireless sensor networks|Linear Systems Theory|Secondary Control for the Safety of LTI Systems under Attacks|Secure-by-Construction Controller Synthesis via Control Barrier Functions|Dynamic System Fault Diagnosis Under Sparseness Assumption|', 'citations': '', 's2FieldsOfStudy': 'Computer Science|Engineering|Engineering|Computer Science|', 'tldr': 'This work derives an exact characterization of the set of all possible state estimates in order to deal with potential state estimation uncertainty in linear systems when an omniscient attacker can spoof several system sensors at will.'}\n",
      "{'hash_id': 948081209, 'title': 'On the Hardness of Meaningful Local Guarantees in Nonsmooth Nonconvex Optimization', 'authors': 'Guy Kornowski, Swati Padmanabhan, Ohad Shamir', 'published': datetime.datetime(2024, 9, 16, 14, 35, tzinfo=datetime.timezone.utc), 'summary': 'We study the oracle complexity of nonsmooth nonconvex optimization, with the\\nalgorithm assumed to have access only to local function information. It has\\nbeen shown by Davis, Drusvyatskiy, and Jiang (2023) that for nonsmooth\\nLipschitz functions satisfying certain regularity and strictness conditions,\\nperturbed gradient descent converges to local minimizers asymptotically.\\nMotivated by this result and by other recent algorithmic advances in nonconvex\\nnonsmooth optimization concerning Goldstein stationarity, we consider the\\nquestion of obtaining a non-asymptotic rate of convergence to local minima for\\nthis problem class.\\n  We provide the following negative answer to this question: Local algorithms\\nacting on regular Lipschitz functions cannot, in the worst case, provide\\nmeaningful local guarantees in terms of function value in sub-exponential time,\\neven when all near-stationary points are global minima. This sharply contrasts\\nwith the smooth setting, for which it is well-known that standard gradient\\nmethods can do so in a dimension-independent rate. Our result complements the\\nrich body of work in the theoretical computer science literature that provide\\nhardness results conditional on conjectures such as $\\\\mathsf{P}\\\\neq\\\\mathsf{NP}$\\nor cryptographic assumptions, in that ours holds unconditional of any such\\nassumptions.', 'pdf_url': 'http://arxiv.org/pdf/2409.10323v1', 'entry_id': 'http://arxiv.org/abs/2409.10323v1', 'recommended': 1, 'referenceCount': 50, 'citationCount': 0, 'references': 'The Complexity of Computing KKT Solutions of Quadratic Programs|Goldstein stationarity in Lipschitz constrained optimization|An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization|Deterministic Nonsmooth Nonconvex Optimization|Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion|No dimension-free deterministic algorithm computes approximate stationarities of Lipschitzians|The Cost of Nonconvexity in Deterministic Nonsmooth Optimization|Gradient-Free Methods for Deterministic and Stochastic Nonsmooth Nonconvex Optimization|On the Finite-Time Complexity and Practical Computation of Approximate Stationarity Concepts of Lipschitz Functions|A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions|Active manifolds, stratifications, and convergence to local minima in nonsmooth optimization|Stochastic Subgradient Descent Escapes Active Strict Saddles on Weakly Convex Functions|Escaping strict saddle points of the Moreau envelope in nonsmooth optimization|Proximal Methods Avoid Active Strict Saddles of Weakly Convex Functions|Oracle Complexity in Nonsmooth Nonconvex Optimization|Escaping Saddle Points for Nonsmooth Weakly Convex Functions via Perturbed Proximal Algorithms|The complexity of gradient descent: CLS = PPAD ∩ PLS|On the complexity of finding a local minimizer of a quadratic function over a polytope|Complexity of Finding Stationary Points of Nonconvex Nonsmooth Functions|Pathological Subgradient Dynamics|On Nonconvex Optimization for Machine Learning|Provably Correct Automatic Subdifferentiation for Qualified Programs|Stochastic Subgradient Method Converges on Tame Functions|Stochastic model-based minimization of weakly convex functions|Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems|Stochastic Methods for Composite and Weakly Convex Optimization Problems|How to Escape Saddle Points Efficiently|Escaping From Saddle Points - Online Stochastic Gradient for Tensor Decomposition|Curves of Descent|Continuous local search|Stochastic Approximations and Differential Inclusions|Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization|Open questions in complexity theory for numerical optimization|Checking local optimality in constrained quadratic programming is NP-hard|Some NP-complete problems in quadratic and nonlinear programming|Generalized gradients of Lipschitz functionals|Generalized Directional Derivatives and Subgradients of Nonconvex Functions|Optimization of lipschitz continuous functions|Computationally Related Problems|Some Related Problems from Network Flows, Game Theory and Integer Programming|Über partielle und totale differenzierbarkeit von Funktionen mehrerer Variabeln und über die Transformation der Doppelintegrale|On the Hardness of Computing Near-Approximate Stationary Points of Clarke Regular Nonsmooth Nonconvex Problems and Certain DC Programs|Modern nonconvex nondifferentiable optimization|Deep Learning|Variational analysis . Vol. 317|An Elementary Introduction to Modern Convex Geometry|Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization|Optimization And Nonsmooth Analysis|Problem Complexity and Method Efficiency in Optimization|Minimization of nondifferentiable functions in the presence of noise|', 'citations': '', 's2FieldsOfStudy': 'Mathematics|Computer Science|Mathematics|Computer Science|', 'tldr': 'A negative answer to the question of obtaining a non-asymptotic rate of convergence to local minima for local algorithms acting on regular Lipschitz functions cannot, in the worst case, provide meaningful local guarantees in terms of function value in sub-exponential time, even when all near-stationary points are global minima.'}\n"
     ]
    }
   ],
   "source": [
    "helper = ArxivResearchHelper()\n",
    "papers = helper.search_papers_aug(query, max_results=paper_num, date_from=date_from, date_to=date_to)\n",
    "for paper in papers:\n",
    "    paper['query_id'] = query_id \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab06b0b3-bcc0-44d4-836e-922a8e2b971c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T05:47:19.5842059Z",
       "execution_start_time": "2024-11-10T05:47:19.3544826Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "80046781-fc0e-484f-bec1-c6e9c9d7ec17",
       "queued_time": "2024-11-10T05:44:33.9197817Z",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 669cd7cd-0251-4696-a1f4-95f98e0976de, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def saveAsTable(source, name):\n",
    "    df = pd.DataFrame(source)\n",
    "    spark_df = spark.createDataFrame(df,schema=schema)\n",
    "    spark_df.write.format(\"delta\").saveAsTable(name)\n",
    "\n",
    "def appendTable(source,name):\n",
    "    df = pd.DataFrame(source)\n",
    "    spark_df = spark.createDataFrame(df,schema=schema)\n",
    "    spark_df.write.mode(\"append\").format(\"delta\").saveAsTable(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff95d4e-65ec-48e5-9d90-05247921435c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T05:47:45.2832448Z",
       "execution_start_time": "2024-11-10T05:47:19.9846357Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a622226e-2d10-4a80-a6de-e1921055c548",
       "queued_time": "2024-11-10T05:44:36.775751Z",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, 669cd7cd-0251-4696-a1f4-95f98e0976de, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "t = str(time.time()).split(\".\")[0]\n",
    "query_name = re.sub(r'[^a-zA-Z]', '', query)\n",
    "filename = f\"bronze._{query_id}_{query_name}\"\n",
    "appendTable(papers,filename)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69735877-b9de-4b6f-a762-cb2ce19d5a37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-10T05:51:35.1161405Z",
       "execution_start_time": "2024-11-10T05:50:43.3109765Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "53ba11ce-dbcd-41e3-93a1-e98af902c493",
       "queued_time": "2024-11-10T05:50:42.9256445Z",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, 669cd7cd-0251-4696-a1f4-95f98e0976de, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.mssparkutilsrun-result+json": {
       "artifact_id": "bf45a4d2-3c58-46bb-aef7-0f9bdad2bf79",
       "capacity_id": "6C5685BD-E7E1-4DB2-AF07-E027EB0A967D",
       "error": null,
       "in_pipeline": false,
       "notebook_name": "arxiv_tag",
       "root_artifact_id": "289b3aae-bcfa-44f4-aa72-92f5ff7dd7d5",
       "run_id": "b89e4b32-8797-465e-8e05-08fb8c772674",
       "session_id": "669cd7cd-0251-4696-a1f4-95f98e0976de",
       "snapshot_error": null,
       "snapshot_path": "",
       "snapshot_status": "success",
       "spark_pool": "Starter Pool",
       "workspace_id": "2007662d-6654-4a40-be93-23217fe4b693"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mssparkutils.notebook.run(\"arxiv_tag\", 10000, {\"filename\": filename })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4f407-def6-4a50-9023-cd1ba8dffb26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74fd6b-fb93-4d91-adaa-ea0c979da56e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "0089b3bb-3e3f-4921-b047-ba87ec130fb4",
    "default_lakehouse_name": "bronze",
    "default_lakehouse_workspace_id": "2007662d-6654-4a40-be93-23217fe4b693",
    "known_lakehouses": [
     {
      "id": "e2d3ab7d-46e8-42d0-8a83-29ad04b04aeb"
     },
     {
      "id": "0089b3bb-3e3f-4921-b047-ba87ec130fb4"
     },
     {
      "id": "49f908d1-4437-4cc0-b70d-f2ee89449202"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
