{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab75e93a-e656-4c2e-84d3-978cc0f0d651",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-12T20:48:41.4262993Z",
       "execution_start_time": "2024-11-12T20:48:41.1865095Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "003b073e-7c55-440a-a3b6-da4cfda7382b",
       "queued_time": "2024-11-12T20:48:40.7210107Z",
       "session_id": "9383db11-2fc4-4b33-8c3b-cbc6f91ab628",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 9383db11-2fc4-4b33-8c3b-cbc6f91ab628, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import hmac\n",
    "import base64\n",
    "from datetime import datetime , timezone\n",
    "import azure.cosmos\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, TimestampType, LongType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"query_text\", StringType(), True),\n",
    "    StructField(\"date_from\", StringType(), True),\n",
    "    StructField(\"date_to\", StringType(), True),\n",
    "    StructField(\"priority\", LongType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"status\", LongType(), True),\n",
    "    StructField(\"metadata.created_at\", TimestampType(), True),\n",
    "    StructField(\"metadata.updated_at\", TimestampType(), True),\n",
    "    StructField(\"metadata.source\", StringType(), True),\n",
    "    StructField(\"partitionKey\", StringType(), True),\n",
    "    StructField(\"updated_at\", StringType(), True),\n",
    "    StructField(\"_rid\", StringType(), True),\n",
    "    StructField(\"_self\", StringType(), True),\n",
    "    StructField(\"_etag\", StringType(), True),\n",
    "    StructField(\"_attachments\", StringType(), True),\n",
    "    StructField(\"_ts\", LongType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9d68ec-aee2-46b7-b74d-d8f86f815e71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-12T20:47:21.3738958Z",
       "execution_start_time": "2024-11-12T20:47:21.0948113Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6f3360dd-16e6-4ddd-bb64-edb0aa2c5afb",
       "queued_time": "2024-11-12T20:46:06.1808123Z",
       "session_id": "9383db11-2fc4-4b33-8c3b-cbc6f91ab628",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 9383db11-2fc4-4b33-8c3b-cbc6f91ab628, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from email.policy import default\n",
    "\n",
    "from azure.cosmos import CosmosClient, PartitionKey, exceptions\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class CosmosDBClient:\n",
    "    def __init__(self, url: str, key: str, database_name: str, container_name: str, partition_key: str):\n",
    "        \"\"\"\n",
    "        Initialize the CosmosDBClient with Cosmos DB URL, key, database, container, and partition key.\n",
    "\n",
    "        Parameters:\n",
    "            url (str): Cosmos DB endpoint URL.\n",
    "            key (str): Cosmos DB primary key for authorization.\n",
    "            database_name (str): Name of the Cosmos DB database.\n",
    "            container_name (str): Name of the Cosmos DB container.\n",
    "            partition_key (str): Partition key path for the container.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.key = key\n",
    "        self.database_name = database_name\n",
    "        self.container_name = container_name\n",
    "        self.partition_key = partition_key\n",
    "\n",
    "        # Initialize Cosmos Client and connect to database and container\n",
    "        try:\n",
    "            self.client = CosmosClient(self.url, credential=self.key)\n",
    "            self.database = self._create_database_if_not_exists()\n",
    "            self.container = self._create_container_if_not_exists()\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Failed to connect to Cosmos DB: {e}\")\n",
    "\n",
    "    def _create_database_if_not_exists(self):\n",
    "        \"\"\"\n",
    "        Create the database if it does not exist.\n",
    "\n",
    "        Returns:\n",
    "            DatabaseProxy: A reference to the Cosmos DB database.\n",
    "        \"\"\"\n",
    "        return self.client.create_database_if_not_exists(id=self.database_name)\n",
    "\n",
    "    def _create_container_if_not_exists(self):\n",
    "        \"\"\"\n",
    "        Create the container if it does not exist with a specified partition key.\n",
    "\n",
    "        Returns:\n",
    "            ContainerProxy: A reference to the Cosmos DB container.\n",
    "        \"\"\"\n",
    "        return self.database.create_container_if_not_exists(\n",
    "            id=self.container_name,\n",
    "            partition_key=PartitionKey(path=f\"/{self.partition_key}\"),\n",
    "            offer_throughput=400  # Set the desired throughput\n",
    "        )\n",
    "\n",
    "    def create_document(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Insert a document into the container. ID will be autogenerated if not provided.\n",
    "\n",
    "        Parameters:\n",
    "            data (Dict[str, Any]): Document data to insert.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: The created document, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data.setdefault(self.partition_key, \"default_partition\")  # Ensure partition key exists\n",
    "            document = self.container.create_item(body=data)\n",
    "            print(\"Document created successfully.\")\n",
    "            return document\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error creating document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def upsert_document(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Upsert (update or insert) a document in the container.\n",
    "\n",
    "        Parameters:\n",
    "            data (Dict[str, Any]): Document data to upsert.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: The upserted document, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data.setdefault(self.partition_key, \"default_partition\")\n",
    "            document = self.container.upsert_item(body=data)\n",
    "            print(\"Document upserted successfully.\")\n",
    "            return document\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error upserting document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def read_document(self, document_id: str, partition_key: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Read a document by ID.\n",
    "\n",
    "        Parameters:\n",
    "            document_id (str): The ID of the document to read.\n",
    "            partition_key (str): The partition key of the document.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: The retrieved document, or None if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            document = self.container.read_item(item=document_id, partition_key=partition_key)\n",
    "            return document\n",
    "        except exceptions.CosmosResourceNotFoundError:\n",
    "            print(\"Document not found.\")\n",
    "            return None\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error reading document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def query_documents(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Query documents in the container.\n",
    "\n",
    "        Parameters:\n",
    "            query (str): SQL query string to execute.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of documents that match the query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            items = list(self.container.query_items(query=query, enable_cross_partition_query=True))\n",
    "            return items\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error querying documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def delete_document(self, document_id: str, partition_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete a document by ID.\n",
    "\n",
    "        Parameters:\n",
    "            document_id (str): The ID of the document to delete.\n",
    "            partition_key (str): The partition key of the document.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the document was deleted, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.container.delete_item(item=document_id, partition_key=partition_key)\n",
    "            print(\"Document deleted successfully.\")\n",
    "            return True\n",
    "        except exceptions.CosmosResourceNotFoundError:\n",
    "            print(\"Document not found.\")\n",
    "            return False\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error deleting document: {e}\")\n",
    "            return False\n",
    "\n",
    "    def list_all_documents(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List all documents in the container.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of all documents in the container.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            items = list(self.container.read_all_items())\n",
    "            return items\n",
    "        except exceptions.CosmosHttpResponseError as e:\n",
    "            print(f\"Error listing all documents: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ed5a31-000d-46e7-acd0-56766e2bfbfe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-12T20:52:30.9932246Z",
       "execution_start_time": "2024-11-12T20:52:23.0486583Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a1667df6-4d06-4eb2-877a-cdd86868a7a4",
       "queued_time": "2024-11-12T20:52:22.631135Z",
       "session_id": "9383db11-2fc4-4b33-8c3b-cbc6f91ab628",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, 9383db11-2fc4-4b33-8c3b-cbc6f91ab628, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "COSMOS_URL = ''\n",
    "COSMOS_KEY = \"\"\n",
    "DATABASE_NAME = ''\n",
    "CONTAINER_NAME = ''\n",
    "PARTITION_KEY = ''  # Ensure this matches the partition key path in your Cosmos DB\n",
    "\n",
    "USER_ID = \"\"\n",
    "\n",
    "cosmos_client_query = CosmosDBClient(\n",
    "    url=COSMOS_URL,\n",
    "    key=COSMOS_KEY,\n",
    "    database_name=DATABASE_NAME,\n",
    "    container_name=CONTAINER_NAME,\n",
    "    partition_key=PARTITION_KEY\n",
    ")\n",
    "def appendTable(source,name):\n",
    "    spark_df = spark.createDataFrame(source,schema)\n",
    "    spark_df.write.mode(\"append\").format(\"delta\").saveAsTable(name)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(\"SELECT * FROM query.query_pending  WHERE status=0 ORDER BY priority LIMIT 1000\")\n",
    "except:\n",
    "    appendTable([],\"query_pending\")\n",
    "    df = spark.sql(\"SELECT * FROM query.query_pending  WHERE status=0 ORDER BY priority LIMIT 1000\")\n",
    "\n",
    "\n",
    "df_pd = df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f056e47d-2473-4a0a-ae11-2b493cdfb50f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-11-12T20:52:00.5205207Z",
       "execution_start_time": "2024-11-12T20:51:59.7495113Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6d7b4f9c-dcf8-4f3d-8ac7-26dc03c07b08",
       "queued_time": "2024-11-12T20:51:50.8818093Z",
       "session_id": "9383db11-2fc4-4b33-8c3b-cbc6f91ab628",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, 9383db11-2fc4-4b33-8c3b-cbc6f91ab628, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document upserted successfully.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 515fd3b5-d45e-4ab8-b6f5-2475daf90db5).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- hash_id: long (nullable = true)\n-- title: string (nullable = true)\n-- authors: string (nullable = true)\n-- published: timestamp (nullable = true)\n-- summary: string (nullable = true)\n-- pdf_url: string (nullable = true)\n-- entry_id: string (nullable = true)\n-- recommended: long (nullable = true)\n-- referenceCount: long (nullable = true)\n-- citationCount: long (nullable = true)\n-- references: string (nullable = true)\n-- citations: string (nullable = true)\n-- s2FieldsOfStudy: string (nullable = true)\n-- tldr: string (nullable = true)\n-- query_id: string (nullable = true)\n-- Tag_1: string (nullable = true)\n-- Tag_2: string (nullable = true)\n-- Tag_3: string (nullable = true)\n-- Tag_4: string (nullable = true)\n-- Tag_5: string (nullable = true)\n-- field: string (nullable = true)\n\n\nData schema:\nroot\n-- id: string (nullable = true)\n-- user_id: string (nullable = true)\n-- query_text: string (nullable = true)\n-- date_from: string (nullable = true)\n-- date_to: string (nullable = true)\n-- priority: long (nullable = true)\n-- email: string (nullable = true)\n-- status: long (nullable = true)\n-- metadata.created_at: timestamp (nullable = true)\n-- metadata.updated_at: timestamp (nullable = true)\n-- metadata.source: string (nullable = true)\n-- partitionKey: string (nullable = true)\n-- _rid: string (nullable = true)\n-- _self: string (nullable = true)\n-- _etag: string (nullable = true)\n-- _attachments: string (nullable = true)\n-- _ts: long (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m         cosmos_client_query\u001b[38;5;241m.\u001b[39mupsert_document(document)\n\u001b[1;32m     17\u001b[0m df_pd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 19\u001b[0m appendTable(df_pd,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery.query_processing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS query_pending\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m empty_pending \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([], schema)\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mappendTable\u001b[0;34m(source, name)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappendTable\u001b[39m(source,name):\n\u001b[1;32m     19\u001b[0m     spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(source,schema)\n\u001b[0;32m---> 20\u001b[0m     spark_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[0;32m~/cluster-env/clonedenv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 515fd3b5-d45e-4ab8-b6f5-2475daf90db5).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- hash_id: long (nullable = true)\n-- title: string (nullable = true)\n-- authors: string (nullable = true)\n-- published: timestamp (nullable = true)\n-- summary: string (nullable = true)\n-- pdf_url: string (nullable = true)\n-- entry_id: string (nullable = true)\n-- recommended: long (nullable = true)\n-- referenceCount: long (nullable = true)\n-- citationCount: long (nullable = true)\n-- references: string (nullable = true)\n-- citations: string (nullable = true)\n-- s2FieldsOfStudy: string (nullable = true)\n-- tldr: string (nullable = true)\n-- query_id: string (nullable = true)\n-- Tag_1: string (nullable = true)\n-- Tag_2: string (nullable = true)\n-- Tag_3: string (nullable = true)\n-- Tag_4: string (nullable = true)\n-- Tag_5: string (nullable = true)\n-- field: string (nullable = true)\n\n\nData schema:\nroot\n-- id: string (nullable = true)\n-- user_id: string (nullable = true)\n-- query_text: string (nullable = true)\n-- date_from: string (nullable = true)\n-- date_to: string (nullable = true)\n-- priority: long (nullable = true)\n-- email: string (nullable = true)\n-- status: long (nullable = true)\n-- metadata.created_at: timestamp (nullable = true)\n-- metadata.updated_at: timestamp (nullable = true)\n-- metadata.source: string (nullable = true)\n-- partitionKey: string (nullable = true)\n-- _rid: string (nullable = true)\n-- _self: string (nullable = true)\n-- _etag: string (nullable = true)\n-- _attachments: string (nullable = true)\n-- _ts: long (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for _id, user_id in zip(df_pd[\"id\"], df_pd[\"user_id\"]):\n",
    "    # Query to get the specific document by its ID and user_id (partition key)\n",
    "    query = f\"SELECT * FROM c WHERE c.id = '{_id}' AND c.user_id = '{user_id}'\"\n",
    "    documents = cosmos_client_query.query_documents(query=query)\n",
    "    \n",
    "    if documents:\n",
    "        document = documents[0]  # Assuming each ID corresponds to a single document\n",
    "        \n",
    "        # Update the status and add a timestamp\n",
    "        document['status'] = 1\n",
    "        document['updated_at'] = datetime.now(timezone.utc).isoformat()  # Optional timestamp\n",
    "        \n",
    "        # Upsert the updated document back into Cosmos DB\n",
    "        cosmos_client_query.upsert_document(document)\n",
    "\n",
    "\n",
    "df_pd[\"status\"] =1\n",
    "\n",
    "appendTable(df_pd,\"query.query_processing\")\n",
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS query_pending\")\n",
    "empty_pending = spark.createDataFrame([], schema)\n",
    "appendTable([],\"query_pending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6607b-fd0f-4beb-a19b-1ea8d2622c3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "COSMOS_URL = ''\n",
    "COSMOS_KEY = \"\"\n",
    "DATABASE_NAME = ''\n",
    "CONTAINER_NAME = ''\n",
    "PARTITION_KEY = ''  # Ensure this matches the partition key path in your Cosmos DB\n",
    "\n",
    "USER_ID = \"\"\n",
    "\n",
    "cosmos_client_query = CosmosDBClient(\n",
    "    url=COSMOS_URL,\n",
    "    key=COSMOS_KEY,\n",
    "    database_name=DATABASE_NAME,\n",
    "    container_name=CONTAINER_NAME,\n",
    "    partition_key=PARTITION_KEY\n",
    ")\n",
    "def appendTable(source,name):\n",
    "    spark_df = spark.createDataFrame(source,schema)\n",
    "    spark_df.write.mode(\"append\").format(\"delta\").saveAsTable(name)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(\"SELECT * FROM query.query_pending  WHERE status=0 ORDER BY priority LIMIT 1000\")\n",
    "except:\n",
    "    appendTable([],\"query_pending\")\n",
    "    df = spark.sql(\"SELECT * FROM query.query_pending  WHERE status=0 ORDER BY priority LIMIT 1000\")\n",
    "\n",
    "\n",
    "df_pd = df.toPandas()\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "d51d0c2b-441e-4917-92b3-112dfe252df2",
    "workspaceId": "2007662d-6654-4a40-be93-23217fe4b693"
   },
   "lakehouse": {
    "default_lakehouse": "49f908d1-4437-4cc0-b70d-f2ee89449202",
    "default_lakehouse_name": "query",
    "default_lakehouse_workspace_id": "2007662d-6654-4a40-be93-23217fe4b693"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
